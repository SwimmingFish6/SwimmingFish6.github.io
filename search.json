[{"title":"K8S学习系列（一）：K8S的基本设计","url":"/2020/05/29/kubernetes_design/","content":"\nKubernetes的设计理念\n===\nKubernetes作为目前容器编排领域的几乎垄断型选手，在学习Cloud Native方面是很重要的一环，因此今天开个系列，来记录一下K8S的学习过程。今天主要的内容还是搞清楚K8S的架构和大体上的各个组件和基本功能，之后如果有时间再深入其他的缓解，主要的参考资料是 `https://jimmysong.io/kubernetes-handbook`\n\n# Kubernetes的基本结构\n借用handbook里面的一张图，就描述了整个Kubernetes的大致架构和各个组件之间的交互。主要的部分有两个，一个就是master component，一个是Node。\n![avatar](https://jimmysong.io/kubernetes-handbook/images/kubernetes-high-level-component-archtecture.jpg)\n\n在Master Component上，主要的组件是etcd， scheduler，controller manager和api server，负责集群状态管理，资源调度，负载均衡等等。而Node也就是K8S真正要管理的资源单位。一个Node下面可以有多个pod。具体的细节我们在本章节后面讨论。在这里本人有个思考，因为kubernetes被称为云操作系统，而Linux则是本地操作系统,如果能将两个系统对比,会不会有比较因缺思厅的东西发生呢?于是我找了两个分层结构图的对比:\n\nLinux的分层结构：\n\n![avatar](https://tecadmin.net/tutorial/wp-content/uploads/2017/10/linux-architecture-image.png)\n\nK8S的分层结构：\n![avatar](https://jimmysong.io/kubernetes-handbook/images/kubernetes-layers-arch.png)\n\nK8S的结构分为核心层（对外提供API构建应用，对内提供插件式应用），应用层（部署，路由和servicemesh部分），管理层（系统度量，自动化，policy和servicemesh部分），接口层（命令行工具）和生态系统（包括外部的集成应用和内部的资源CRI、CNI、CSI、image registry和cloud provider等）。\n\n如果我们把这个对应到linux的图中，应该是Hardware对应K8S调配的资源，Kernal对应API层，也就是一系列的system library帮助我们在资源上搭建应用，本质上来说，K8S的应用层和管理也应该属于Kernal，因为它做的是一些networking和scheduler的工作。然后到了接口层，很显而易见的对应shell，最后在此之上搭建了appliction。\n\n总结一下，两个模型对应关系分别是Hardware->Cloud Computing Resource，Kernal对应核心层，应用层加管理层，Shell对应接口层，Application对应应用层。\n\n# 设计原则\n\n## API\n在K8S集群中，每添加一项新功能，就等于引入了对应的API对象。API设计的原则主要有如下几条：\n\n* **声明式。** 声明式同时也是Cloud Native的一个原则，目的就是防止系统状态出现不一，同时隐藏系统具体完成API的过程。比较典型的一个例子就是replica的数量。如果是声明式，那就直接告诉系统replica是3个，如果是命令式，则需要告诉系统从x到3的过程，包括增减replica。前者的好处就是API调用的过程中已经包含了调用者想要系统最后达到的状态，有助于系统状态的维护，分布式里非常重要。\n\n* **互补可组合。** 对于这一点的理解就是“高内聚，松耦合”。提高对象可重用性。\n\n* **高层API从操作意图出发。** 这一点我的理解就是API的设计需要和操作的意图相结合，让你能够将操作对应到API上，而不是需要比较复杂的API组合才能完成一个操作。\n\n* **低层API根据高层控制需要设计。** 低层API由于不需要被用户直接使用，则是需要根据被高层使用时的可重用性，减少冗余方面考虑。\n\n* **避免简单封装。** 这一点，我的理解是，对于一些封装逻辑很简单的API我们没必要强行组合它们。我们需要让操作者知道这个API对什么对象做了什么事情，而不是简单封装它们而导致API意图不清。\n\n* **API操作复杂度。** 这点很好理解，毕竟是Cloud Native，伸缩性和水平扩展，API操作复杂度不能超过O(N)，不然就没有意义了。\n\n* **API对象状态不依赖网络状态。** API对象在分布式的环境下，即使遭遇网络断开，也能保持稳定的状态。（状态包括？）\n\n* **操作机制不依赖全局状态。** 毕竟分布式环境下全局状态完全同步比较困难，即使还未同步我们也希望它能可用。\n\n## 控制机制设计原则\n* 只依赖当前状态\n* 假设任何错误的可能，并做容错物理\n* 控制逻辑不能依赖无法监控的内部状态（意思就是可控的，不能说一个子系统状态变化对另一个子系统的状态影响是不可预测的）\n* 假设任何操作都可能被任何对象拒绝或者错误解析。（即使这样，也不能影响系统的稳定性）\n* 模块自我修复能力\n* 模块优雅降级（保证基本服务可用性）\n\n# 核心技术概念\n\n## API对象\n每个功能对应的会有API对象，比如Replica Set对应RS。每个API对象有三大类属性：metadata， spec和status。metadata至少包括namespace，name和uid，还有labels标识对象类别，比如env标识不同环境下的API对象。Spec描述了理想状态，status描述了当前状态。比如pod副本数理想状态是3，当前状态是2。下面是一些比较重要的API对象\n\n## Pod\nPod是K8S中运行部署应用或服务的最小单元，支持多容器。多个容器会在一个pod下面共享网络服务和文件系统。比如一个对外发布的网络服务，一个可能会向外发布，一个可能会拉取新的信息，两个容器被不同的team开发，最终放在一个pod下组成一个微服务。\n\n## Replication Controller（RC）\nRC是最早保证Pod高可用的API对象，在K8S比较早期实现，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。\n\n## Replica Set（RS）\nRS是新一代RC，同样提供高可用，区别是提供更多匹配样式。一般作为Deployment的理想参数使用。\n\n## Deployment\n部署其实是一个复合操作，包括更新新服务，创建新服务，滚动升级等。滚动升级相当于是RS将新服务的副本数目增长，减少旧服务这样一个复合操作。\n\n## Service\n因为Pod本身所在的物理位置是并不确定的，因此Service提供了服务发现和负载均衡的能力，在Pod更新的过程中同时进行服务IP和端口的更新，保证高可用。负载均衡在K8S中是通过Kube-proxy实现的，一个Node上面一个。\n\n## Job\nJob是K8S用来控制批处理型任务的API对象。批处理的任务是有头有尾的。成功完成的标志根据不同的spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。\n\n## DaemonSet\n后台服务主要要保证每个Node都有一个此类的Pod在运行，包括存储，日志和监控等。\n\n## StatefulSet\n有状态服务集，主要是针对一些有状态的应用，即，即使Pod挂了，我们也希望用同样的名字将其恢复到原来的状态，包括挂载同样的存储（意味着主要使用外挂存储），主要针对Mysql等DB服务，Zookeeper，etcd等状态管理服务等。同时可以作为一种虚拟机保存，因为Container的状态保存其实是很不安全和不可靠的。\n\n## Federation\n主要针对跨地区的kubernetes cluster再进行集群管理，K8S cluster可以注册成为联邦的一份子。不影响cluster内部的逻辑。\n\n## Volume\nKubernetes集群中的存储卷跟Docker的存储卷有些类似，只不过Docker的存储卷作用范围为一个容器，而Kubernetes的存储卷的生命周期和作用范围是一个Pod。每个Pod中声明的存储卷由Pod中的所有容器共享。\n\n## Persistent Volume / Persistent Volume Claim\nPV和PVC的关系与Node和Pod的关系类似。PV和Node是资源提供者，PVC和Pod是资源使用者。PV和PVC使得K8S cluster的存储抽象化。\n\n## Node\nNode就是Pod所在的主机，无论是物理机还是虚拟机。\n\n## Secret\nSecret对象用来传递密码，密钥，证书等。\n\n## User Account / Service Account\nUser Account标识用户，Service Account标识服务。\n\n## Namespace\nNamespace为K8S Cluster提供虚拟隔离，初始namespace包括default和kube-system。\n\n## RBAC\n权限控制，不必多说，所有权限控制系统都叫这个名字。\n","tags":["K8S"]},{"url":"/2020/05/24/sessions-in-microservices/"},{"title":"手撕Docker系列-第一章","url":"/2020/04/25/chapter-1/","content":"\n手撕Docker系列-第一章\n===\n之前看到第一章是介绍go和docker相关知识就没有过多深入，今天有空看了看，觉得一些东西还是值得记录，因此写了这一章。\n\n# Docker\n\n能看到这篇文章的想必对docker也有一定了解，不必要的话就无需讲了。Docker作为virtualization的核心技术，可以将应用程序以及相应的依赖资源打包成一个标准的镜像，并以容器的方式运行在任何支持docker engine的系统上。Docker总体采用的是典型的C/S结构，并没有涉及很多底层或者分布式系统的东西。具体架构图如下。\n\n![avatar](https://wiki.aquasec.com/download/attachments/2854889/Docker_Architecture.png?version=1&modificationDate=1520172700553&api=v2)\n\n可以看到比较核心的是Docker Deamon，此外客户端和docker的交互就是通过api的形式进行。具体Deamon和其他组件做了什么，在后面的章节我们具体学习。\n\nDocker的优势主要有三个，\n\n- 轻量级：在同一台宿主机的容器共享系统的kernal，我们无需再搭建一个OS，因此启动速度快（秒级启动），占用系统内存少。又因为AUFS使得镜像之间能够通过分层结构共享文件，提高了磁盘的利用率和镜像下载速度。\n- 开放：Docker容器基于开放标准，因此Docker可以在主流Linux和windows操作系统上运行。\n- 安全：通过Namespace达到了资源的隔离，docker之间无法相互干扰，提供了额外的保障机制。\n\n# Docker和VM\n同样是模拟出一个独立的操作系统环境，VM虚拟机经常拿来和Docker进行比较。\n![avatar](https://wiki.aquasec.com/download/attachments/2854889/Container_VM_Implementation.png?version=1&modificationDate=1520172703952&api=v2)\n\n这个图就很明显了。VM的核心技术集中于Hypervisor上，Hypervisor更像是一个软件，基于OS的基础上，模拟出各种硬件的行为（包括CPU，硬盘等）。在Hypervisor的基础上，我们再搭建OS。这个缺点很明显，就是我们每需要打开一个虚拟机，我们就需要在Hypervisor上安装一个OS，动辄就是几个GB。\n\n而Docker克服了VM的缺点，对开发人员开发效率来说，主要有三个帮助：\n\n- 加速开发：Docker Registry提供了各种标准化的镜像，同时也提供了开发者定制镜像的能力，再也无需花费很久重新设置环境。\n- 赋能创造力：对这一点，我理解为，由于启动一个docker成本低而且docker之间相互不干扰，使得我们可以为每一个程序设置最好的环境，避免依赖之间相互冲突，以及复杂的版本管理。\n- 消除环境不一致：Docker的标准使得我们开发不用受开发环境的限制，无论在什么环境下达到开箱即用的效果。\n\n此外，Docker Hub的存在使得一个团队可以通过共享镜像的方式实现协作开发。此外，docker秒级启动的特性能让服务迅速扩容。","tags":["Docker"]},{"title":"手撕Docker系列-第三章","url":"/2020/04/25/chapter-3/","content":"\n手撕Docker系列-第三章\n===\n今天在这个章节我们会讲如何应用我们前一章的知识，来创建一个容器进程。\n\n# Linux proc文件系统\n\nLinux的/proc目录其实不是一个真正的文件系统，因为真正的文件系统通过一系列的metadata对磁盘上的文件进行管理。而/proc下的内容包含了系统runtime的metadata，包括系统内存，mount设备以及一些硬件配置。它只存在于内存中，而不占用外存空间。事实上，它只是提供了一个接口让用户以访问文件的形式访问这些信息。\n\n|  目录   | 内容  |\n|  ----  | ----  |\n| /proc/N | PID为N的进程信息 |\n| /proc/N/cmdline | 进程启动命令 |\n| /proc/N/cwd | 链接到进程当前工作目录 |\n| /proc/N/environ | 进程环境变量列表 |\n| /proc/N/exe | 链接到进程的执行命令文件 |\n| /proc/N/fd | 包含进程相关的所有文件描述符 |\n| /proc/N/maps | 与进程相关的内存映射信息 |\n| /proc/N/mem | 指代进程持有的内存(不可读) |\n| /proc/N/root | 连接到进程的根目录 |\n| /proc/N/stat | 进程状态 |\n| /proc/N/statm | 进程使用的进程状态 |\n| /proc/N/status | 进程状态信息，比stat/statm更具可读性 |\n| /proc/self/ | 链接到当前正在运行的进程 |\n\n# Run命令启动\n\nmydocker\n├─README.md\n├─main.go\n├─main_command.go\n├─run.go\n├─network\n&nbsp;|&emsp;└test_linux.go\n├─container\n&nbsp;|&emsp;├─container_process.go\n&nbsp;|&emsp;└init.go\n├─Godeps\n&nbsp;|&emsp;├─Godeps.json\n&nbsp;|&emsp;└Readme\n\n文件目录如上所示。在这里我们不贴出过多的细节，主要把容器的启动过程分为几个部分。\n\n## run\nrun的过程里，最主要的定义了一些运行的flag（tty）。`Run(tty, cmd)`是关键的部分，这部分代码有必要贴出来。\n\n```\nfunc Run(tty bool, command string) {\n\tparent := container.NewParentProcess(tty, command)\n\tif err := parent.Start(); err != nil {\n\t\tlog.Error(err)\n\t}\n\tparent.Wait()\n\tos.Exit(-1)\n}\n\nfunc NewParentProcess(tty bool, command string) *exec.Cmd {\n\targs := []string{\"init\", command}\n\tcmd := exec.Command(\"/proc/self/exe\", args...)\n    cmd.SysProcAttr = &syscall.SysProcAttr{\n        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS |\n\t\tsyscall.CLONE_NEWNET | syscall.CLONE_NEWIPC,\n    }\n\tif tty {\n\t\tcmd.Stdin = os.Stdin\n\t\tcmd.Stdout = os.Stdout\n\t\tcmd.Stderr = os.Stderr\n\t}\n\treturn cmd\n}\n```\n在`container.NewParentProcess(tty, command)`中，mydocker启动了一个进程`/proc/self/exe`，这个在我们之前的段落已经有提及，就是设置了一个进程变量，进程的可执行文件为当前进程的可执行文件。还有一些第二章提到的Namespace来进行资源隔离。包括UTS（hostname），PID（进程号），NS（挂载点），NET（网络资源）和IPC（进程交互通道）。在之后又设置了一些输入输出的通道。可能看到它调用自己会有点疑惑，其实更清楚的思路是，在创建这个进程的时候也传入了一个参数`init`，相当于调用./mydocker init。我们再看看init干了什么。\n\n## init\n在init的过程中，主要是init了container的process，最关键的一个函数叫做`container.RunContainerinitProcess(cmd, nil)`。\n\n```\nfunc RunContainerInitProcess(command string, args []string) error {\n\tlogrus.Infof(\"command %s\", command)\n\n\tdefaultMountFlags := syscall.MS_NOEXEC | syscall.MS_NOSUID | syscall.MS_NODEV\n\tsyscall.Mount(\"proc\", \"/proc\", \"proc\", uintptr(defaultMountFlags), \"\")\n\targv := []string{command}\n\tif err := syscall.Exec(command, argv, os.Environ()); err != nil {\n\t\tlogrus.Errorf(err.Error())\n\t}\n\treturn nil\n}\n```\n\n在这个函数里创建的进程本质上没有干什么事情。而函数本身首先规定了一个挂载参数，然后挂载到/proc上。之后再通过exec启动。\n\nMountFlags的意义如下\n* MS_NOEXEC： 这个文件系统中不允许运行其它程序。\n* MS_NOSUID: 这个文件系统下不允许设置user_id或者group id。\n* MS_NODEV: Linux2.0默认设置参数。\n\nexec这句语句也很关键，看起来它只是简单执行了一个程序。但是其实背后有比较复杂的逻辑。首先，当我们运行完run命令之后，我们希望暴露给我们的前台进程是容器进程，然后目前为止PID为1的前台进程仍然是init进程，而syscall.Exec这个方法， 其实最终调用了Kernel的intexecve(const char filename,char *const argv[], char *const envp[]);这个系统函数会执行对应文件，并覆盖当前进程的镜像，堆栈和数据，包括PID。\n\n在之后，我们容器已经启动了，我们可以通过`ps -ef`去查看目前的进程号是否为1。\n\n# 增加容器资源限制\n在这个部分，我们希望能够让mydocker实现资源限制。e.g. `mydocker run -ti -m IOOm -cpuset I -cpushare 512 /bin/sh`。其实在经历过第二章原理以后，这部分的实现相当容易。首先，假设我们在这里只考虑memory的限制，我们要做的就是创建一个memory subsystem。在这个memory subsystem中，我们将限制的变量写入memory挂载点下面指定的Cgroup。具体的subsystem的挂载点可以通过/proc/self/mountinfo来进行查看，值得注意的是，mountinfo里面得到的并不是文件系统下的绝对路径，我们仍然需要通过拼接等得到绝对路径。之后将限制参数写入Cgroup下的文件里。最后我们通过将进程加入挂载点下的指定Cgroup中来限制进程资源的使用。具体的代码实现可以看书中的实现\n\n![avatar](https://docs.google.com/drawings/d/e/2PACX-1vR4tG0VAHY4bizgADLvOKWP_olEh5NMrS0_D0BeMQVmx7gabMJaqdB8xrtg_RqQunad32VTwAkCG5iw/pub?w=960&h=720)\n\n# 增加管道和环境变量识别\n首先在这个章节我们考虑一个问题，就是在容器中父子进程的通信。其实在初始化的过程中，父子进程就已经有一次通讯了。也就是我们需要启动`mydocker init --args`的时候，我们传给子进程的参数，包括init command和参数。当出现参数太长或者有特殊字符串的时候，这种办法就会失败。事实上runC采用的就是匿名管道的方法进行通信。我们在这里需要增加一个函数`NewPipe()`。\n\n```\nfunc NewPipe() (*os.File, *os.File, error) {\n\tread, write, err := os.Pipe()\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn read, write, nil\n}\n\nfunc NewParentProcess(tty bool) (*exec.Cmd, *os.File) {\n\treadPipe, writePipe, err := NewPipe()\n\tif err != nil {\n\t\tlog.Errorf(\"New pipe error %v\", err)\n\t\treturn nil, nil\n\t}\n\tcmd := exec.Command(\"/proc/self/exe\", \"init\")\n\tcmd.SysProcAttr = &syscall.SysProcAttr{\n\t\tCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS |\n\t\t\tsyscall.CLONE_NEWNET | syscall.CLONE_NEWIPC,\n\t}\n\tif tty {\n\t\tcmd.Stdin = os.Stdin\n\t\tcmd.Stdout = os.Stdout\n\t\tcmd.Stderr = os.Stderr\n\t}\n\tcmd.ExtraFiles = []*os.File{readPipe}\n\treturn cmd, writePipe\n}\n```\n\nNewPipe通过os创建了一个匿名管道用于父子进程交互。readPipe作为了cmd.ExtraFiles参数传给了init进程，此时，一个进程便拥有了四个文件句柄：标准输入，标准输出，标准错误以及这个readPipe。而写句柄则是传到外部，并将需要运行的command写入，从而让init进程能够读到这些参数。在这之后writePipe就被close了。\n\n# 总结\n至此，我们为进程添加了隔离环境，资源限制以及管道机制，基本上实现了一个容器进程的运行。","tags":["Docker"]},{"title":"手撕Docker系列-第二章","url":"/2020/03/24/chapter-2/","content":"\n手撕Docker系列-第二章\n===\n终于开始写博客啦，希望能够通过这个过程让自己能够沉淀下来，更好地深入技术吧。因为最近入职于一家SAAS公司，平时工作里不可避免的要碰到k8s和docker，趁此机会也好好熟悉这两门技术。主要的学习手段是通过陈显鹭前辈的这本《自己动手写Docker》开始，一方面是实现一个东西可以让人对它了解更深刻，另一方面是熟悉go语言的使用，毕竟是工作语言...借此机会系统地学一下。\n\n这一章主要分为三个部分，一个是namespace，一个是Cgroup，还有就是AUFS。作为docker虚拟化概念中的核心技术。\n\n# Namespce\nNamespace是Linux的Kernel的一个功能，通过这个手段，Linux可以将其，我们在这里称之为各种资源隔离开来，其中包括进程，包括UserID，还有Network等。这也是docker镜像之间能够相互不干扰的原理所在。\n\nNamespace根据资源类型，我们可以将其分为好几种，具体的代码可以见我的github，也可以直接参考《自己动手写Docker》。在Linux中一共实现了6种不同类型的Namespace\n\n## UTS Namespace\n\nUTS Namespace用于隔离nodename和domainname，前者就是所谓的主机名，后者就是域名。在创建新的UTS namespace之后内部hostname独立于外部。\n\n## IPC Namespace\n\nIPC Namespace用于隔离System V IPC和POSIX message queues，前者就是Unix早期进程间通信的所有集合，包括管道（同时包括有名管道）、信号、消息队列、共享内存、信号量。后者是提供了实现POSIX标准的消息队列。\n\n## PID Namespace\n\nPID Namespace是用来隔离进程ID的。要注意这里是进程ID不是进程.同一个进程在不同PID Namespace可以拥有不同的PID。\n\n## Mount Namespace\n\nMount Namespace用来隔离各个进程看到的挂载点视图。首先比较难理解的是挂载，在这里，我将其解释为将文件系统和目录树结合在一起的一种结构，相当于将一个磁盘挂载至一个挂载点后，你就可以通过文件目录访问磁盘，并且文件系统也提供了inode、block等信息，更多的是一个关于怎么管理这片磁盘区域的配置信息。\n\n## User Namespace\n\nUser Namespace相对来说就好理解很多。众所周知，在Linux里面每个User有自己的UID和GroupID，后者规定了用户所在的用户组（权限控制相关）。因此User Namespace就是来划分这个UID和GroupID的。在不同的User Namespace中，不同的用户可以有不同的UID和GroupID，而且在不同User Namespace中的ID也是没有关联的，比如UID=1在两个User Namespace中就是完全相互独立的。\n\n## Network Namespace\n\n这个书里讲得很清楚。Network Namespace就是用来隔离网络设备，IP端口等网络栈的命名空间。每个Network Namespace中可以拥有独立的虚拟网络设备和自己的端口，且与其他的Network Namespace不冲突。\n\n# Cgroup\n\nLinux Cgroup，全程Control Group。它的出现为的是解决一个什么问题呢？之前的Namespace，针对的只是Namespace之间的隔离。而我们如果要求能够对资源进行限制呢？现有的Namespace是无法做到这点的。于是Linux中就引入了Cgroup这个概念。Cgroup针对的是进程，相当于是一个进程分组框架。在Cgroup中主要有两个概念：hierarchy和subsystem。\n\n## Hierarchy\n\nHierachy表达的是一个层次结构，Hierachy是一个树状结构，而在一个Hierachy下，可以绑定多个子Hierachy。这样就实现的进程的层级控制，所以一棵树更多的是相当于将进程进行分组。\n\n## Subsystem\n\n首先声明一个约束，一个subsystem只能挂载到一个Cgroup Hierachy节点上。而这个subsystem可以根据约束的资源，分为9种类型：\n+ cpu subsystem （CPU使用率）\n+ cpuacct subsystem（进程的CPU使用报告）\n+ cpuset subsystem（为进程分配单独的CPU节点或者内存节点）\n+ memory subsystem（内存分配）\n+ blkio subsystem（设备io资源分配）\n+ devices subsystem（设备访问控制）\n+ net_cls subsystem（标记Cgroup下的进程数据包，使用tc模块（traffic control）进行数据包控制）\n+ freezer subsystem （挂起恢复进程）\n+ ns subsystem （使得不同cgroup下面的进程使用不同的namespace）\n\n（net_cls和freezer还不是很懂）\n\n# AUFS\n之前的两个概念一个帮助docker容器之间相互隔离，一个帮助docker分配和限制系统资源。AUFS（Advanced Multi-Layered Unification Filesyste）则是用来高效节省空间和文件复用的。书中的cases由于编码的原因比较难理解，我在此会用比较简单的方式表达。首先docker会管理本地仓库的一堆images。比如，我pull了一个ubuntu 14.0.0的image 1，这个image由四层image layer组成。然后基于这个image，我又创建了一个docker image 2\n\n```\nFFROM image 1\n\nRUN sh-cmd\n```\n\n## Image Layer\n在这个sh-cmd中，有可能一些文件被更改了，但是其实大部分都是可以复用的，而在/var/lib/docker/aufs/diff/下面存了所有的layer，在这里，我们假设image 1有3个layer： layer1， layer2， layer3。而/var/lib/docker/aufs/layers则是存储了layer的metadata。比如在layer3下包括了layer1和layer2.在我们创建了新的image 2后。我们新添加了一个layer，而相比较image1，这个image其实并没有发生很大变大，因此我们只需要将他们的diff存在layer4，而image 2则由layer1，layer2，layer3，layer4组成。当访问不到相关文件，就会去下层的layer寻找。在书中的例子中，layer4的大小仅为12B大大节省了空间。\n\n## Container Layer\nContainer Layer则是用于管理container创建以后的管理.当一个container创建之后，会用到一个技术，被称为写时复制(copy on write)。也就是说，当且仅当这个container对文件进行写操作的时候，文件才会从下层Layer复制上来。而这个缺点则是，即使文件有很小的改动，也需要复制整个文件，好处就是，可以让文件尽可能的服用和节省磁盘空间。Container创建的时候会同事创建两个layer，一个是layer—id-init，另一个则是layer-id。前者是read-only的，存一些关于这个docker镜像的环境相关的数据，另一个则是read-write层，用于完成之前我提到的CoW技术。Container的metadata存在/var/lib/containers/container_id，包括容器的metadata和一些config。\n\n而关于删除一个文件file1，则是在read-write层添加一个.wh.file1，这样就可以屏蔽这个层以下所有的read-only层的file1文件。\n\n到此，这个章节也告一段落。","tags":["Docker"]},{"title":"MIT 6.828--Lab1","url":"/2019/04/30/MIT 6.828-Lab1/","content":"\n\n## Section 1\n\n\n\n\n## Section 2\n![Aaron Swartz](https://github.com/SwimmingFish6/MDImageResource/raw/master/elf.jpg)\n","tags":["OS"]},{"title":"Hello World","url":"/2019/04/30/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]