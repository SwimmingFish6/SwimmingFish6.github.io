[{"title":"手撕Docker系列-第一章","url":"/2020/04/25/chapter-1/","content":"\n手撕Docker系列-第一章\n===\n之前看到第一章是介绍go和docker相关知识就没有过多深入，今天有空看了看，觉得一些东西还是值得记录，因此写了这一章。\n\n# Docker\n\n能看到这篇文章的想必对docker也有一定了解，不必要的话就无需讲了。Docker作为virtualization的核心技术，可以将应用程序以及相应的依赖资源打包成一个标准的镜像，并以容器的方式运行在任何支持docker engine的系统上。Docker总体采用的是典型的C/S结构，并没有涉及很多底层或者分布式系统的东西。具体架构图如下。\n\n![avatar](https://wiki.aquasec.com/download/attachments/2854889/Docker_Architecture.png?version=1&modificationDate=1520172700553&api=v2)\n\n可以看到比较核心的是Docker Deamon，此外客户端和docker的交互就是通过api的形式进行。具体Deamon和其他组件做了什么，在后面的章节我们具体学习。\n\nDocker的优势主要有三个，\n\n- 轻量级：在同一台宿主机的容器共享系统的kernal，我们无需再搭建一个OS，因此启动速度快（秒级启动），占用系统内存少。又因为AUFS使得镜像之间能够通过分层结构共享文件，提高了磁盘的利用率和镜像下载速度。\n- 开放：Docker容器基于开放标准，因此Docker可以在主流Linux和windows操作系统上运行。\n- 安全：通过Namespace达到了资源的隔离，docker之间无法相互干扰，提供了额外的保障机制。\n\n# Docker和VM\n同样是模拟出一个独立的操作系统环境，VM虚拟机经常拿来和Docker进行比较。\n![avatar](https://wiki.aquasec.com/download/attachments/2854889/Container_VM_Implementation.png?version=1&modificationDate=1520172703952&api=v2)\n\n这个图就很明显了。VM的核心技术集中于Hypervisor上，Hypervisor更像是一个软件，基于OS的基础上，模拟出各种硬件的行为（包括CPU，硬盘等）。在Hypervisor的基础上，我们再搭建OS。这个缺点很明显，就是我们每需要打开一个虚拟机，我们就需要在Hypervisor上安装一个OS，动辄就是几个GB。\n\n而Docker克服了VM的缺点，对开发人员开发效率来说，主要有三个帮助：\n\n- 加速开发：Docker Registry提供了各种标准化的镜像，同时也提供了开发者定制镜像的能力，再也无需花费很久重新设置环境。\n- 赋能创造力：对这一点，我理解为，由于启动一个docker成本低而且docker之间相互不干扰，使得我们可以为每一个程序设置最好的环境，避免依赖之间相互冲突，以及复杂的版本管理。\n- 消除环境不一致：Docker的标准使得我们开发不用受开发环境的限制，无论在什么环境下达到开箱即用的效果。\n\n此外，Docker Hub的存在使得一个团队可以通过共享镜像的方式实现协作开发。此外，docker秒级启动的特性能让服务迅速扩容。","tags":["Docker"]},{"title":"手撕Docker系列-第三章","url":"/2020/04/25/chapter-3/","content":"\n手撕Docker系列-第三章\n===\n\n# Linux proc文件系统\n\nLinux的/proc目录其实不是一个真正的文件系统，因为真正的文件系统通过一系列的metadata对磁盘上的文件进行管理。而/proc下的内容包含了系统runtime的metadata，包括系统内存，mount设备以及一些硬件配置。它只存在于内存中，而不占用外存空间。事实上，它只是提供了一个接口让用户以访问文件的形式访问这些信息。\n\n|  目录   | 内容  |\n|  ----  | ----  |\n| /proc/N | PID为N的进程信息 |\n| /proc/N/cmdline | 进程启动命令 |\n| /proc/N/cwd | 链接到进程当前工作目录 |\n| /proc/N/environ | 进程环境变量列表 |\n| /proc/N/exe | 链接到进程的执行命令文件 |\n| /proc/N/fd | 包含进程相关的所有文件描述符 |\n| /proc/N/maps | 与进程相关的内存映射信息 |\n| /proc/N/mem | 指代进程持有的内存(不可读) |\n| /proc/N/root | 连接到进程的根目录 |\n| /proc/N/stat | 进程状态 |\n| /proc/N/statm | 进程使用的进程状态 |\n| /proc/N/status | 进程状态信息，比stat/statm更具可读性 |\n| /proc/self/ | 链接到当前正在运行的进程 |\n\n# Run命令启动\n\nmydocker<br/>\n├─README.md<br/>\n├─main.go<br/>\n├─main_command.go<br/>\n├─run.go<br/>\n├─network<br/>\n&nbsp;|&emsp;└test_linux.go<br/>\n├─container<br/>\n&nbsp;|&emsp;├─container_process.go<br/>\n&nbsp;|&emsp;└init.go<br/>\n├─Godeps<br/>\n&nbsp;|&emsp;├─Godeps.json<br/>\n&nbsp;|&emsp;└Readme<br/>\n\n文件目录如上所示。在这里我们不贴出过多的细节，主要把容器的启动过程分为几个部分。\n\n## run\nrun的过程里，最主要的定义了一些运行的flag（tty）。`Run(tty, cmd)`是关键的部分，这部分代码有必要贴出来。\n\n```\nfunc Run(tty bool, command string) {\n\tparent := container.NewParentProcess(tty, command)\n\tif err := parent.Start(); err != nil {\n\t\tlog.Error(err)\n\t}\n\tparent.Wait()\n\tos.Exit(-1)\n}\n\nfunc NewParentProcess(tty bool, command string) *exec.Cmd {\n\targs := []string{\"init\", command}\n\tcmd := exec.Command(\"/proc/self/exe\", args...)\n    cmd.SysProcAttr = &syscall.SysProcAttr{\n        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS |\n\t\tsyscall.CLONE_NEWNET | syscall.CLONE_NEWIPC,\n    }\n\tif tty {\n\t\tcmd.Stdin = os.Stdin\n\t\tcmd.Stdout = os.Stdout\n\t\tcmd.Stderr = os.Stderr\n\t}\n\treturn cmd\n}\n```\n在`container.NewParentProcess(tty, command)`中，mydocker启动了一个进程`/proc/self/exe`，这个在我们之前的段落已经有提及，就是设置了一个进程变量，进程的可执行文件为当前进程的可执行文件。还有一些第二章提到的Namespace来进行资源隔离。包括UTS（hostname），PID（进程号），NS（挂载点），NET（网络资源）和IPC（进程交互通道）。在之后又设置了一些输入输出的通道。可能看到它调用自己会有点疑惑，其实更清楚的思路是，在创建这个进程的时候也传入了一个参数`init`，相当于调用./mydocker init。我们再看看init干了什么。\n\n## init\n在init的过程中，主要是init了container的process，最关键的一个函数叫做`container.RunContainerinitProcess(cmd, nil)`。\n\n```\nfunc RunContainerInitProcess(command string, args []string) error {\n\tlogrus.Infof(\"command %s\", command)\n\n\tdefaultMountFlags := syscall.MS_NOEXEC | syscall.MS_NOSUID | syscall.MS_NODEV\n\tsyscall.Mount(\"proc\", \"/proc\", \"proc\", uintptr(defaultMountFlags), \"\")\n\targv := []string{command}\n\tif err := syscall.Exec(command, argv, os.Environ()); err != nil {\n\t\tlogrus.Errorf(err.Error())\n\t}\n\treturn nil\n}\n```\n\n在这个函数里创建的进程本质上没有干什么事情。而函数本身首先规定了一个挂载参数，然后挂载到/proc上。之后再通过exec启动。\n\nMountFlags的意义如下\n* MS_NOEXEC： 这个文件系统中不允许运行其它程序。\n* MS_NOSUID: 这个文件系统下不允许设置user_id或者group id。\n* MS_NODEV: Linux2.0默认设置参数。\n\nexec这句语句也很关键，看起来它只是简单执行了一个程序。但是其实背后有比较复杂的逻辑。首先，当我们运行完run命令之后，我们希望暴露给我们的前台进程是容器进程，然后目前为止PID为1的前台进程仍然是init进程，而syscall.Exec这个方法， 其实最终调用了Kernel的intexecve(const char filename,char *const argv[], char *const envp[]);这个系统函数会执行对应文件，并覆盖当前进程的镜像，堆栈和数据，包括PID。\n\n在之后，我们容器已经启动了，我们可以通过`ps -ef`去查看目前的进程号是否为1。\n\n# 增加容器资源限制\n在这个部分，我们希望能够让mydocker实现资源限制。e.g. `mydocker run -ti -m IOOm -cpuset I -cpushare 512 /bin/sh`。其实在经历过第二章原理以后，这部分的实现相当容易。首先，假设我们在这里只考虑memory的限制，我们要做的就是创建一个memory subsystem。在这个memory subsystem中，我们将限制的变量写入memory挂载点下面指定的Cgroup。具体的subsystem的挂载点可以通过/proc/self/mountinfo来进行查看，值得注意的是，mountinfo里面得到的并不是文件系统下的绝对路径，我们仍然需要通过拼接等得到绝对路径。之后将限制参数写入Cgroup下的文件里。最后我们通过将进程加入挂载点下的指定Cgroup中来限制进程资源的使用。具体的代码实现可以看书中的实现\n\n![avatar](https://docs.google.com/drawings/d/e/2PACX-1vR4tG0VAHY4bizgADLvOKWP_olEh5NMrS0_D0BeMQVmx7gabMJaqdB8xrtg_RqQunad32VTwAkCG5iw/pub?w=960&h=720)\n\n# 增加管道和环境变量识别\n首先在这个章节我们考虑一个问题，就是在容器中父子进程的通信。其实在初始化的过程中，父子进程就已经有一次通讯了。也就是我们需要启动`mydocker init --args`的时候，我们传给子进程的参数，包括init command和参数。当出现参数太长或者有特殊字符串的时候，这种办法就会失败。事实上runC采用的就是匿名管道的方法进行通信。我们在这里需要增加一个函数`NewPipe()`。\n\n```\nfunc NewPipe() (*os.File, *os.File, error) {\n\tread, write, err := os.Pipe()\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn read, write, nil\n}\n\nfunc NewParentProcess(tty bool) (*exec.Cmd, *os.File) {\n\treadPipe, writePipe, err := NewPipe()\n\tif err != nil {\n\t\tlog.Errorf(\"New pipe error %v\", err)\n\t\treturn nil, nil\n\t}\n\tcmd := exec.Command(\"/proc/self/exe\", \"init\")\n\tcmd.SysProcAttr = &syscall.SysProcAttr{\n\t\tCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS |\n\t\t\tsyscall.CLONE_NEWNET | syscall.CLONE_NEWIPC,\n\t}\n\tif tty {\n\t\tcmd.Stdin = os.Stdin\n\t\tcmd.Stdout = os.Stdout\n\t\tcmd.Stderr = os.Stderr\n\t}\n\tcmd.ExtraFiles = []*os.File{readPipe}\n\treturn cmd, writePipe\n}\n```\n\nNewPipe通过os创建了一个匿名管道用于父子进程交互。readPipe作为了cmd.ExtraFiles参数传给了init进程，此时，一个进程便拥有了四个文件句柄：标准输入，标准输出，标准错误以及这个readPipe。而写句柄则是传到外部，并将需要运行的command写入，从而让init进程能够读到这些参数。在这之后writePipe就被close了。\n\n# 总结\n至此，我们为进程添加了隔离环境，资源限制以及管道机制，基本上实现了一个容器进程的运行。","tags":["Docker"]},{"title":"手撕Docker系列-第二章","url":"/2020/03/24/chapter-2/","content":"\n手撕Docker系列-第二章\n===\n终于开始写博客啦，希望能够通过这个过程让自己能够沉淀下来，更好地深入技术吧。因为最近入职于一家SAAS公司，平时工作里不可避免的要碰到k8s和docker，趁此机会也好好熟悉这两门技术。主要的学习手段是通过陈显鹭前辈的这本《自己动手写Docker》开始，一方面是实现一个东西可以让人对它了解更深刻，另一方面是熟悉go语言的使用，毕竟是工作语言...借此机会系统地学一下。\n\n这一章主要分为三个部分，一个是namespace，一个是Cgroup，还有就是AUFS。作为docker虚拟化概念中的核心技术。\n\n# Namespce\nNamespace是Linux的Kernel的一个功能，通过这个手段，Linux可以将其，我们在这里称之为各种资源隔离开来，其中包括进程，包括UserID，还有Network等。这也是docker镜像之间能够相互不干扰的原理所在。\n\nNamespace根据资源类型，我们可以将其分为好几种，具体的代码可以见我的github，也可以直接参考《自己动手写Docker》。在Linux中一共实现了6种不同类型的Namespace\n\n## UTS Namespace\n\nUTS Namespace用于隔离nodename和domainname，前者就是所谓的主机名，后者就是域名。在创建新的UTS namespace之后内部hostname独立于外部。\n\n## IPC Namespace\n\nIPC Namespace用于隔离System V IPC和POSIX message queues，前者就是Unix早期进程间通信的所有集合，包括管道（同时包括有名管道）、信号、消息队列、共享内存、信号量。后者是提供了实现POSIX标准的消息队列。\n\n## PID Namespace\n\nPID Namespace是用来隔离进程ID的。要注意这里是进程ID不是进程.同一个进程在不同PID Namespace可以拥有不同的PID。\n\n## Mount Namespace\n\nMount Namespace用来隔离各个进程看到的挂载点视图。首先比较难理解的是挂载，在这里，我将其解释为将文件系统和目录树结合在一起的一种结构，相当于将一个磁盘挂载至一个挂载点后，你就可以通过文件目录访问磁盘，并且文件系统也提供了inode、block等信息，更多的是一个关于怎么管理这片磁盘区域的配置信息。\n\n## User Namespace\n\nUser Namespace相对来说就好理解很多。众所周知，在Linux里面每个User有自己的UID和GroupID，后者规定了用户所在的用户组（权限控制相关）。因此User Namespace就是来划分这个UID和GroupID的。在不同的User Namespace中，不同的用户可以有不同的UID和GroupID，而且在不同User Namespace中的ID也是没有关联的，比如UID=1在两个User Namespace中就是完全相互独立的。\n\n## Network Namespace\n\n这个书里讲得很清楚。Network Namespace就是用来隔离网络设备，IP端口等网络栈的命名空间。每个Network Namespace中可以拥有独立的虚拟网络设备和自己的端口，且与其他的Network Namespace不冲突。\n\n# Cgroup\n\nLinux Cgroup，全程Control Group。它的出现为的是解决一个什么问题呢？之前的Namespace，针对的只是Namespace之间的隔离。而我们如果要求能够对资源进行限制呢？现有的Namespace是无法做到这点的。于是Linux中就引入了Cgroup这个概念。Cgroup针对的是进程，相当于是一个进程分组框架。在Cgroup中主要有两个概念：hierarchy和subsystem。\n\n## Hierarchy\n\nHierachy表达的是一个层次结构，Hierachy是一个树状结构，而在一个Hierachy下，可以绑定多个子Hierachy。这样就实现的进程的层级控制，所以一棵树更多的是相当于将进程进行分组。\n\n## Subsystem\n\n首先声明一个约束，一个subsystem只能挂载到一个Cgroup Hierachy节点上。而这个subsystem可以根据约束的资源，分为9种类型：\n+ cpu subsystem （CPU使用率）\n+ cpuacct subsystem（进程的CPU使用报告）\n+ cpuset subsystem（为进程分配单独的CPU节点或者内存节点）\n+ memory subsystem（内存分配）\n+ blkio subsystem（设备io资源分配）\n+ devices subsystem（设备访问控制）\n+ net_cls subsystem（标记Cgroup下的进程数据包，使用tc模块（traffic control）进行数据包控制）\n+ freezer subsystem （挂起恢复进程）\n+ ns subsystem （使得不同cgroup下面的进程使用不同的namespace）\n\n（net_cls和freezer还不是很懂）\n\n# AUFS\n之前的两个概念一个帮助docker容器之间相互隔离，一个帮助docker分配和限制系统资源。AUFS（Advanced Multi-Layered Unification Filesyste）则是用来高效节省空间和文件复用的。书中的cases由于编码的原因比较难理解，我在此会用比较简单的方式表达。首先docker会管理本地仓库的一堆images。比如，我pull了一个ubuntu 14.0.0的image 1，这个image由四层image layer组成。然后基于这个image，我又创建了一个docker image 2\n\n```\nFFROM image 1\n\nRUN sh-cmd\n```\n\n## Image Layer\n在这个sh-cmd中，有可能一些文件被更改了，但是其实大部分都是可以复用的，而在/var/lib/docker/aufs/diff/下面存了所有的layer，在这里，我们假设image 1有3个layer： layer1， layer2， layer3。而/var/lib/docker/aufs/layers则是存储了layer的metadata。比如在layer3下包括了layer1和layer2.在我们创建了新的image 2后。我们新添加了一个layer，而相比较image1，这个image其实并没有发生很大变大，因此我们只需要将他们的diff存在layer4，而image 2则由layer1，layer2，layer3，layer4组成。当访问不到相关文件，就会去下层的layer寻找。在书中的例子中，layer4的大小仅为12B大大节省了空间。\n\n## Container Layer\nContainer Layer则是用于管理container创建以后的管理.当一个container创建之后，会用到一个技术，被称为写时复制(copy on write)。也就是说，当且仅当这个container对文件进行写操作的时候，文件才会从下层Layer复制上来。而这个缺点则是，即使文件有很小的改动，也需要复制整个文件，好处就是，可以让文件尽可能的服用和节省磁盘空间。Container创建的时候会同事创建两个layer，一个是layer—id-init，另一个则是layer-id。前者是read-only的，存一些关于这个docker镜像的环境相关的数据，另一个则是read-write层，用于完成之前我提到的CoW技术。Container的metadata存在/var/lib/containers/container_id，包括容器的metadata和一些config。\n\n而关于删除一个文件file1，则是在read-write层添加一个.wh.file1，这样就可以屏蔽这个层以下所有的read-only层的file1文件。\n\n到此，这个章节也告一段落。","tags":["Docker"]},{"title":"MIT 6.828--Lab1","url":"/2019/04/30/MIT 6.828-Lab1/","content":"\n\n## Section 1\n\n\n\n\n## Section 2\n![Aaron Swartz](https://github.com/SwimmingFish6/MDImageResource/raw/master/elf.jpg)\n","tags":["OS"]},{"title":"Hello World","url":"/2019/04/30/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]