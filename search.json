[{"title":"悲观并发控制和乐观并发控制","url":"/2020/12/26/concurrency_control/","content":"\n# 悲观并发控制\n悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）。\n\n## 出发点\n假定所有事务所操作的数据都会受到其他事务的冲突，所以每次在获取数据前都会加锁。\n\n## 适用场景\n数据写入比较频繁，竞争激烈的场景，应用于很多金融行业场景。在这种场景下可以避免大量无效的尝试。\n\n## 实现方式\n行锁，表锁等，读锁，写锁\n\n# 乐观并发控制\n乐观并发控制（又名“乐观锁”，Optimistic Concurrency Control，缩写“OCC”）\n\n## 出发点\n假定所有的事务所操作的数据都相互不影响，每次事务提交数据更新前，会检查所获取的数据有没有被别人更新。\n\n## 适用场景\n读操作频繁的场景。可以避免因为加锁带来的大量无效等待。\n\n## 实现方式\n版本号和时间戳，有的db会提供类似write_condition机制。","tags":["Distributed System","Database"]},{"title":"TiDB调度核心PD原理","url":"/2020/12/25/PD/","content":"\nTiDB调度核心PD原理解读\n===\n# PD职能\nPD负责TiDB全局元信息的存储，以及TiKV集群负载均衡调度，通过嵌入etcd来支持分布式和容错\n\n## 关键组件\n\n* Store： 集群的存储节点，tikv-server实例，严格一一对应。\n\n* **Region**： 每个Region负责维护一段连续数据，每份数据在不同的Store保存多个副本。\n* **Peer**：每个数据的副本被称为Peer。同一个Region的多个Peer通过raft协议进行同步，所以Peer也代表raft的一个节点，TiKV使用multi-raft模式管理数据。\n* **Raft Group**：每个Region对应一个独立运行的raft实例，这个实例叫做Raft Group。\n\n* **Leader / Follower/ Learner**：\n    * **Leader**：Leader负责响应客户端的读写请求。\n    * **Follower**：Follower被动地从Leader同步数据。\n    * **Learner**：learner它只参与同步raft log而不参与投票，只短暂存在于添加副本的中间步骤。\n\n* **Region Split**：Region不是预先就划分好的，而是随着数据写入到了一定数据量（配置量）逐渐分裂形成的，这个过程被称为Region Split。\n\n* **Pending/Down**：Peer可能出现的两种特殊状态。\n    * **Pending**：Follower或者Learner的raft log和Leader差别很大，此时Follower无法被选为Leader\n    * **Down**：Down是指Leader长时间没有收到对应Peer的消息（宕机或者网络隔离）\n\n* **Scheduler**：PD中调度的组件，PD中每个调度器是独立运行的，不同的调度器如下：\n    * balance-leader-scheduler：保持不同节点的 Leader 均衡。\n    * balance-region-scheduler：保持不同节点的 Peer 均衡。\n    * hot-region-scheduler：保持不同节点的读写热点 Region 均衡。\n    * evict-leader-{store-id}：驱逐某个节点的所有 Leader。（常用于滚动升级）\n* **Operator**：Operator是应用于一个Region的用于调度目的的一系列操作集合。\n* **Operator Step**：\n    * TransferLeader：将 Region Leader 迁移至指定 Peer\n    * AddPeer：在指定 Store 添加 Follower\n    * RemovePeer：删除一个 Region Peer\n    * AddLearner：在指定 Store 添加 Region Learner\n    * PromoteLearner：将指定 Learner 提升为 Follower\n    * SplitRegion：将指定 Region 一分为二\n\n# 调度流程\n1. 信息收集\n    * StoreHeartbeat：Store的基本信息\n    * RegionHeartbeat：Region的基本信息\n2. 生成调度：通过自身逻辑和需求出来生成待执行的Operator\n3. 执行调度：Operator会进入OperatorController管理的等待队列，将每个Operator Step下发给Region的Leader\n\n# Balance\n负载调度的目的是将Region均匀地分散在集群中的所有store上。策略是根据打分从得分高的Store选择Leader或者Peer迁移到得分低的Store。\n## balance-leader\nbalance-leader侧重于分散客户端请求的压力。根据Store上所有Leader对应的region size和作为得分。\n\n## balance-region\nbalance-region侧重于分散存储压力。当空间富余使用数据量计算得分，当空间不足使用剩余空间计算得分。中间态时则加权。\n\n此外，两者的权重在不同性能的Store上可配置。\n\n# 热点调度\n统计持续一段时间读写流量超过阈值的Region，然后分散这些Region。写热点，会同时打散Leader和Peer。读热点，仅打散Leader。\n\n# 集群拓扑感知\n通过调度使得同个Region下的Peer尽可能分散到不同的物理位置。检查这个的组件叫做replicaChecker，依赖于location-labels的配置来进行调度。\n\n# 缩容及故障恢复\n缩容是指预备将某个Store下线，标记为offline状态，故障恢复是指Store发生故障且无法恢复。两种情况下PD需要在其他节点上为这些Region补副本。\n\n# Region Merge\n避免删除数据后大量小Region消耗系统资源。mergeChecker负责，类似于replicaChecker。\n\n# 具体命令和metric监控\n见原文链接：\nhttps://zhuanlan.zhihu.com/p/86173040\n","tags":["TiDB","Distributed System"]},{"title":"Google Spanner Design阅读笔记","url":"/2020/08/15/spanner/","content":"Spanner是google设计的一个可扩展的，面向全球部署的分布式数据库。主要是用来解决高可用性的问题。其主要有如下几个特性。\n1. 数据的replica配置粒度可以精细到什么数据中心包含什么数据，数据到用户的距离（读延迟），不同副本的距离（写延迟），副本数量。\n2. Spanner有两个特性，读和写操作的外部一致性，以及在一个时间戳下的跨越数据库的全球一致性的读操作。\n\n# 实现\n首先我们要澄清几个概念：\n* Universe： universe指的是一个Spanner部署。实际操作中，会有一个devel，stag和prod环境下的universe。\n* Zone：每个Zone是一个BigTable服务器的部署，是管理部署的基本单元。zone的集合也是数据可以被复制到的位置的集合，新的数据中心加入或者老的数据中心被关闭时，zone会被加入一个新的系统，或者移除。一个数据中心中可能运行多个zone。\n![spanner-figure-1](/img/spanner-fig1.jpg)\n上图显示了一个Spanner的Universe的部署架构，包括了多个zone。一个zone包括了一个zonemaster，一百到几千个spanserver。Zonemaster负责把数据分配给spanserver，spanserver将数据提供给客户端。location proxy用来定位可以提供数据的spanserver。Universe master控制zone的各种状态信息。placement drive会周期性和spanserver进行交互，来决定数据转移。\n\n## 软件栈\n![spanner-figure-1](/img/spanner-fig2.jpg)\n每个spanserver敷在管理100-1000个称作tablet的数据结构，实现了`(key:string, timestamp:int64)->string`。timestamp的存在使得其看起来更像一个多版本数据库。tablet状态由一个类似B-树的文件集合和write-ahead日志保存。所有这些会被保存在一个类GFS（Colossus）分布式文件系统保存。\n\n每个tablet上面会维持一个单个Paxos状态机。每个状态机会在相应的tablet中保存metadata和数据。对于每个leader replica而言，每个spanserver会实现一个锁表来实现并发控制（类似实现可以在cockroachdb中看到）。锁表会将键的值域映射到锁状态上面。主要策略是，对于事务性读操作，需要获取锁，其他类型的操作，不需要获取。\n\n每个leader replica，每个spanserver也会实施一个事务管理器来支持分布式事务。如果一个事务只牵扯到一个Paxos group，则可以通过锁表和Paxos二者保证事务性。如果有多个Paxos Group，则会有一个group被选为协调者，该group的leader被称为coordinator leader，slaves被称为coordinator slaves。每个事务管理器的状态会被保存到底层的Paxos group。\n\n## 目录\nSpanner在减值映射的上层实现了一个`目录`的抽象。也就是包含公共前缀的连续键的集合。属于一个目录的所有数据，都具有相同的副本配置。数据转移也是以目录为最小单位。一个Paxos Group可以包含多个目录，这也意味着一个spanner tablet可以使行空间的多个分区。这样，被频繁访问的分区可以整合到一个tablet上。目录也是指定放置策略的最小单位。值得注意的是，当目录变得太大的时候，Spanner会对其进行分片，因此Movedir(后台程序转移目录)在不同的Paxos Group之间转移的是分片。\n\n## 数据模型\nSpanner的数据模型是架构在目录之上的。一个应用会在一个universe中创建一个或者多个数据库。每个数据包含很多表，每个表都具备行列和版本。和关系型数据库不一样的是，Spanner的行必须要有名字，每个表都包含了一个或多个主键列的排序集合：主键形成了一个行的名称，每个表都定义了从主键列到非主键列的映射。当一个行存在时，必须要求给行的其他一些键定义一些值。\n![spanner-figure-1](/img/spanner-fig4.jpg)\n在这个例子中，指定了Users的表层次高于Albums，父节点和子节点存在一对多的关系，因此，子节点，即Albums表，主键必须以父节点的主键为前缀。而所有以根节点的主键作为前缀的记录的集合被称为一个Directory，Directory是分区，复制和迁移的基本单位。一个directory的所有记录因为具有相同的前缀，所以也会被分配到相邻位置。同时，这种数据排列，也方便进行表间的计算，可以直接通过归并的形式实现。\n\n# TrueTime\nTrueTime api返回的事实上是一个时间区间，通过TTinterval结构表达了时间的不确定性。TrueTime可以保证，对于一个调用tt=TT.now()，有tt.earliest≤tabs(e~now)≤tt.latest。具体细节，可以看论文。\n\n# 并发控制\n这个部分论文描述了如何通过TrueTime来保证并发控制的正确性，以及如何实现一些特性。\n![spanner-figure-1](/img/spanner-tab2.jpg)\nSpanner支持读写事务，只读事务和快照读。会被当成读写事务来执行。非快照独立读操作，会被当成只读事务来执行。二者都是在内部进行retry，客户端不用进行这种retry loop。\n\n## 细节\nPending.....比较复杂","tags":["Database","Cloud Native"]},{"title":"Serverless In Wild阅读笔记","url":"/2020/07/19/serverless_policy/","content":"\nServerless in the Wild 阅读笔记\n===\n本文的论文标题是 [`Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider`](https://arxiv.org/pdf/2003.03423.pdf)。\n# 介绍\nFaas（Function as a Service）是非常流行的用于Serverless on the cloud的一种方式。这种方式将资源分配相关的任务全都交给了云端，从而降低了资源的消耗，同时给调用的客户造成了一种service是一直可用的假像。因此这篇论文的目标就是介绍了一种资源管理策略，从而减少资源的cold start和计算资源的消耗。\n\n在这里，我们从三方面来衡量程序能执行多快和资源消耗多少：\n* 首先是程序执行需要把相关code加载到内存，因此如果code在内存里，程序执行就很快。\n* 其次，保存所有程序计算相关资源在内存里，是非常昂贵的。理想的情况是，我们希望能让程序看起来，始终保持warm，然而消耗的资源像是它总是cold的。\n* 最后，不同程序的特征可以差别很大。有些是有固定周期而有些则不。\n\n这篇论文做了两方面的工作：\n* 通过统计分析生产环境workload的特征。在文章的第三章提供了经过清理的生产环境统计数据。\n* 管理cold-start的调用。目前AWS和Azure其实采用了非常naive的策略。就是每次调用完之后将程序保留在内存中固定时间（keep-alive e.g 10 mins）。而本文想要实现的，对不同的workload使用不同的keep-alive。此外，还用了一个pre-warm的变量，预测下次function调用，从而提前加载程序。\n\n# 背景\n## triggers\n这边的triggers即触发程序调用的不同事件类型。在这篇论文中，我们将他们分成7类：\n* HTTP： http调用\n* Event：Azure Event Hub和Azure Event Grid （discret和serial events）\n* Queue：message insertion（Azure Service Bus和Kafka）\n* Timer：类似于cron job（固定时间间隔）\n* Orchestration：Azure Durable Functions\n* Storage：changes in Azure Blob Storage和Redis\n* others\n\n## Applications\n在Azure Function中，function存在的形式是一系列function组成application。事实上被调度的是application而不是function。\n\n## Cold Start\n再补充\n\n## Concurrency和Elasticity\n一个instance事实上可以服务多个request，取决于程序的特性，有的程序有很大的资源需要加载，可能就会造成cold-start很长，但由于这类application的比例小于1%，在本文不考虑这类。\n\n## cold start管理策略\n目前的主流开源和公有云采取的fixed keep-alive策略缺点主要两个：\n* 不同的applications行为表现不同。\n* 固定的模式容易被调用者察觉，恶意调用可以通过固定时间调用程序，从而让程序一直warm，增大程序消耗。\n\n# Faas Workload\n在这一章节，我们就略过了主要的数据分析，直接说结论。\n\n首先，我们发现大概75%的程序的最大的执行时间是10秒，这大概和cold start是一个时间量级的。通俗的说，就是大部分程序调用还是碰到了cold start的问题。\n\n其次，大量的function并没有频繁调用。81%的程序平均最多一分钟调用一次。同时，少于20%的程序承担了大概99.6%的调用。因此，将这些不频繁调用的程序一直放在memory是昂贵的。\n\n最后，40%的application的IAT CV大于1，不需要理解这个参数什么意思，只是说明，就是很大一部分的application调用的间隔变化很大。\n\n# FAAS的Cold Starts管理\n本文提出了一种hybrid histogram policy。这个policy主要包括一系列规则，来控制不同应用的两个属性：\n* Pre-warming window。代表自从上次调用以后，多久会加载这个应用来应对下一次调用。\n* Keep-alive window。这个代表：\n    * 载入内存以后（pre-warming window >= 0）多久会unload\n    * 执行完以后多久会unload （pre-warming window = 0）\n\n## Design Challenge\n### 难预测\n很多application是由timer出发的，因此有固定的pattern。其他类型的application就比较难了。\n\n### application的异质性\none-size-fits-all策略很难适用所有的application。比较好的策略是动态适配不同的应用。\n\n### 不频繁调用的application\n一些应用很少被调用，因此可能需要花比较久的时间去观察它的pattern，对于那些新加入的application也是一样。\n\n### Tracking overhead\n我们也要考虑追踪相关参数所需要花费的overhead。比如，用非常小的数据来保存application的状态。\n\n### Execution overhead\nfunction的执行时间是非常短的，因此我们希望execution的overhead也要很小，不然，缩短function的执行时间就没有意义了。\n\n## Hybrid Histogram Policy\n为了解决挑战一和挑战二，这个策略会适应每个应用的调用频率和模式。每次程序执行完以后，会被立刻从内存unload，然后经过一个pre-warming window以后，加载到内存来应对潜在的调用，之后，保留keep-alive window。如果pre-warming window为0，我们则在keep-alive window以后unload程序。这种情况下会出现三种scenarios。\n\n![scenarios](/img/serverless_scenarios.png)\n\n其实就是一种pre-warm为0，keep-alive总是时间内，下一个invocation来了。一种是pre-warm以后keep-alive时间内，下一个invocation到了，还有一个就是pre-warm和keep alive都过了，invocation才到。\n\n这个策略主要有三个主要组件：\n* range-limited histogram来捕捉每个应用的‘idle time（IT）’\n* 当histogram还不是很有代表性的时候，一个标准的keep-alive方法（不同于fixed policy）\n* 一个时间序列预测的组件（当有太多的IT的时候）。\n![components](/img/serverless_components.png)\n\n### Range-limited histogram\n我们通过一个compact的histogram来追踪IT分布。我们可以设置这个窗口，比如4个小时，而IT的单位则是用分钟表示，这也意味着keep-alive和pre-warm的时间单位也是分钟。这可以在metadata size和policy的准确性之间取得一个平衡。然后我们使用这个histogram的头部作为pre-warm，尾部作为keep-alive。去除一些噪音，我们可以取5%-99%之间的数据作为参考。此外，同时，我们还有一个可配置的margin值。给pre-warm时间减少一个margin%，给keep-alive时间增加一个margin%来减少误差。\n\n### 标准keep-alive\n当application刚刚起步，观察到的IT不够，或者IT分布突然变成了一个新的pattern（not representative），我们开始采用standard keep-alive。这个方法下，pre-warm=0，而keep-alive等于histogram的范围。这个是为了减少在application的pattern还没被观测到的时候cold starts的次数。\n\n一个application不是那么representative的特征是用CV标识。一个single bin有很多的count，其他的大部分都为0。CV就很高，相反，所有bin的值都相同，CV值为0。前者是我们所想要的理想状态。因此我们设置一个threshold，如果CV低于这个，则用standard keep-alive，反之，则用histogram。（Welford's online algorithm追踪）。\n\n### 时间序列分析\n当一个histogram的范围不足以表现application的特征时，我们使用时间序列分析来预测（ARIMA modeling），使用pmdarima package的auto_arima。同样，我们也给预测出的时间一个margin，比如15%。\n\n\n## 在Apache OpenWhisk的实现\n在这个框架中，所有的invocation会经过Controller，来决定发送到哪个invoker。之前的实现是fixed keep-alive window。在这里，使用新policy的实现思路如下：\n* controller。因为所有的invocation都经过controller，我们将histogram和其他一些metadata放在这里。同时更新计算pre-warm和keep-alive时间，并将pre-warm以messga的形式发给invoker。\n* API。我们将keep-alive作为参数，通过invocation request发给invoker，为此增加了一个ActivationMessage的API。\n* Invoker。之前unload container是基于ContainerProxy module的time-out。我们修改这个使其基于keep-alive参数。\n![architecture](/img/openwhisk_architecture.png)\n","tags":["Cloud Native"]},{"title":"K8S学习系列（三）：Pod","url":"/2020/07/10/kubernetes_3/","content":"\nK8S学习系列（三）：Pod\n===\nPod是K8S中调度的基本单位。K8S的基本组件`kube-controller-manager`就是用来控制Pod的状态和生命周期的。本文主要是关于Pod本身的一些概念和生命周期。\n\n# Pod的构成\nPod是K8S里可以创建和部署的最小单位，里面封装应用的容器，储存，独立的网络IP，以及管理容器运行的策略。Pod的使用方式有两种：\n\n* 一个Pod中运行一个容器\n* 在一个Pod中运行多个容器 （容器之间相互耦合协作，共享资源）\n\n当Pod中运行多个进程（即容器）协同工作时，一个pod的容器都会被分到同一个Node上，所有容器共享资源，网络环境和依赖。\n```\n例子： 一个Pod中运行一个web服务器，用到一个Volume，同时一个side容器从远端获取资源更新。\n```\n* 网络\n每个Pod都会被分配一个唯一的IP地址。Pod内的容器在同一个网络环境中，包括端口、IP和localhost。\n\n* 存储\n可以为一个Pod指定多个共享的Volume。\n\n注意区分容器的重启和Pod的重启，容器的重启不会引起pod的重启。Pod不会自愈。此外Pod虽然可以直接访问，但是一般我们都使用controller来控制Pod。\n\n## Controller\nController可以创建和管理多个Pod，提供副本管理，滚动升级和集群级别的自愈能力。一个Node故障，Controller能将Pod调度到其他健康的Node上（事实上Pod并不能移动位置，只是在另一个Node上创建了一个一样的Pod）。\n\n## Question\n* 为什么不在一个容器中运行多个程序？\n    * 透明。方便基础设施对容器进行针对资源监控和进程管理。\n    * 解耦软件依赖。各个容器可以进行版本管理，独立编译和发布。\n    * 使用方便。\n    * 效率。更轻量。\n* 为什么不支持容器的亲和性协同调度\n    * 这样无法使用Pod的许多优点，比如资源共享，IPC，保持状态一致性等。\n\n# Pod的生命周期\n## Init容器\nInit容器是一类在Pod中指定的容器运行前启动的容器。特点如下：\n* Init容器总是运行到成功完成为止。\n* 每个Init容器都必须写在你下一个init容器启动之前完成。\n\n其主要优势如下：\n* 运行实用工具（安全考虑，不建议）\n* 可以使用工具和定制化代码安装。\n* 使用Linux Namespace，与应用程序容器有不同的文件视图。用于访问secret等。\n* 由于是顺序启动的，提供了一种设定一组先决条件的方式。\n\n用途示例：\n* 等待一个service完成\n* Pod注册到远程服务器\n* 启动前等待\n* 克隆Git到数据卷\n* 将动态生成的配置值放到配置文件中。\n\nrestartPolicy决定是否重试。\n\nInit容器由于会被重启，重试，运行应该是幂等的。\n\n## Pause容器\nPause容器，又叫Infra容器，在kubelet中由`KUBELET_POD_INFRA_CONTAINER`指定。\n\n### Pause容器特点\n* 镜像非常小，700kb左右\n* 永远处于Pause状态\n\n### 背景\nPod本身的是一个逻辑概念，解决的核心问题就是，如何让一个Pod中多个被Linux Namespace和cgroups隔开的容器共享某些事情和某些信息。解决这个问题有存储和网络两部分。Pause容器就是为了解决这个问题的网络部分。\n\n### 实现\n一个例子是，在一个Pod中，有两个container，他们共享Network Namespace的方式是，额外起一个Infra container的小容器，来共享整个Pod的Network Namespace。其他容器都会通过Join Namespace的方式加入。所以相当于，其他容器用的IP，Port Mac地址都是这个Infra Container的。因此，整个Pod里面Infra是第一个启动的，整个Pod生命周期等同于Infra Container的生命周期。\n\nKubernetes里的Pause容器主要两个作用：\n* 在pod中担任Linux命名空间共享的基础；\n* 启用pid命名空间，开启init进程\n\n## Pod生命周期\nPod的Phase的可能值：\n* 挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。\n* 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。\n* 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。\n* 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。\n* 未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。\n![avatar](https://jimmysong.io/kubernetes-handbook/images/kubernetes-pod-life-cycle.jpg)\n\n## 容器探针\n探针是由kubelet对容器执行的定期诊断，kubelet通过调用容器实现的Handler，主要有三种类型：\n* ExecAction：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。\n* TCPSocketAction：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。\n* HTTPGetAction：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的。\n\n结果有三类：成功，失败，未知。\n\n而探针则是有两种类型:\n* livenessProbe: 指示容器是否在运行，如果失败则杀死容器，并执行重启策略。\n* readinessProbe：指示容器是否准备好服务。若失败则，endpoint controller会从Pod匹配的所有Service Endpoint中删除该Pod的IP地址。\n\n## Pod hook\nPod hook是由K8S管理的kubelet发起的，在容器启动前或者容器中进程终止之前运行。Hook类型包括：\n* exec：执行一段命令\n* HTTP：发送HTTP请求\n\n## Pod Preset\nPreset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。","tags":["K8S"]},{"title":"Oauth2.0","url":"/2020/07/06/Oauth2/","content":"\nOauth2流程总结\n===\n","tags":["Oauth"]},{"title":"K8S学习系列（四）：服务发现","url":"/2020/07/06/kubernetes_4/","content":"\nK8S学习系列（四）：服务发现\t\n===\n服务发现对于K8S是很重要的一部分。诚然K8S可以很好地管理资源，创建销毁pod，然而pod本身的ip地址是不稳定的，动态的，然而我们对外界暴露的服务需要稳定。因此在这中间，我们需要引入服务发现。\n\n# Service\nK8S的service和pod endpoint这些概念一样，也是K8S中的REST对象。而服务的逻辑意义就是访问一组pod的接口，或者策略。service通常对应微服务的一种服务。既然是一组Pod，那么肯定存在分组策略，在这里我们将这个分组策略称之为Label Selector。而这个selector也不是必须的。Pod的变更通过K8S Endpoints API更新。\n\n# Service的定义\n之前提到Service也是一个REST对象，也就是说，他同样可以通过APIserver做增删改查的rest行为。下面是一个service的定义，port指service自身访问接口，而targetPort指Pod暴露的接口(可以为一个字符串名字)。这个service同时会被指派一个cluster IP。selector会持续对pod进行评估并POST到一个名为`my-service`的Endpoints对象上。\n\n```\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n```\n\n## 没有selector的Service\n没有selector我们可以自己创建一个同名的Endpoints对象（地址不能为 loopback（127.0.0.0/8）、 link-local（169.254.0.0/16）、或者 link-local 多播（224.0.0.0/24））。\n\n```\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-service\nspec:\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n```\n```\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: my-service\nsubsets:\n  - addresses:\n      - ip: 1.2.3.4\n    ports:\n      - port: 9376\n```\n\n## ExternalName\nExternalName Service通过指定externalName，直接在DNS层就发生服务的重定向，不需要任何端口和Endpoint。类似于返回一个外部地址直接访问。\n```\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-service\n  namespace: prod\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n```\n\n# VIP和Service代理\nK8S集群中，每个Node运行一个`kube-proxy`进程。其为每个Service实现了一种VIP的形式。\n\n## userspace代理模式\n可以看到每个Node上面都有一个`kube-proxy`监视kubernetes master对Service和Endpoints对象的添加和移除。当client需要访问某个service的时候，他会安装一个iptables规则，得到service的`clusterIP`和port。对于每个service，`kube-proxy`会开一个随机端口（代理端口），而iptables则是把service请求重定向到这个代理端口，代理端口再转发到Pod。具体分配到哪个Pod的，策略可以有很多不同。\n![avatar](https://jimmysong.io/kubernetes-handbook/images/services-userspace-overview.jpg)\n\n## iptables代理模式\n这个模式主要区别是`kube-proxy`不再负责转发到service，而是由iptables负责。`kube-proxy`负责监视apiserver，获取K8S master对Service和Endpoint西乡的添加和删除，在client需要访问某个service时，通过iptables直接转发到某个backend pod上。对于每个不同的service，它可以安装不同的规则。它的响应更快，没有中间层代理转发。缺点是当一个pod失去响应，它不会再自动帮你转发请求到别的Pod上。\n![avatar](https://jimmysong.io/kubernetes-handbook/images/services-iptables-overview.jpg)\n\n## ipvs模式\n这个是K8S目前使用的模式，与iptables类似，ipvs基于netfilter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着ipvs可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs为负载均衡算法提供了更多选项，例如：\n* rr：轮询调度\n* lc：最小连接数\n* dh：目标哈希\n* sh：源哈希\n* sed：最短期望延迟\n* nq： 不排队调度\n\n![avatar](https://jimmysong.io/kubernetes-handbook/images/service-ipvs-overview.png =100x100)\n\n## 选择自己的IP地址\n可以通过spec.clusterIP来指定service的IP但是必须在service-cluster-ip-range CIDR范围内。在这里之所以我们不用round-robin DNS而是用VIP来找到service，原因是：\n* DNS库的TTL和查询缓存不行\n* 大部分应用只需要查询一次DNS并缓存，如果访问DNS，会给DNS带来很大负载\n\n# 服务发现\n## 环境变量\n一个例子，名为“redis-master”的service：\n```\nREDIS_MASTER_SERVICE_HOST=10.0.0.11\nREDIS_MASTER_SERVICE_PORT=6379\nREDIS_MASTER_PORT=tcp://10.0.0.11:6379\nREDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379\nREDIS_MASTER_PORT_6379_TCP_PROTO=tcp\nREDIS_MASTER_PORT_6379_TCP_PORT=6379\nREDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11\n```\n赋值是通过kublet初始化的时候做的，也就是当pod被创建的时候，上面指定了对应不同服务的一系列环境变量，限制就是Pod想要访问的service必须在Pod之前被创建。\n\n## DNS\nDNS查询很简单，service被创建的时候会向DNS服务器添加DNS记录，“my-service”对应“my-service.my-ns”，也可以附加端口名，例如“_port-name_.my-service.my-ns”来访问特定端口。\n\n# Headless Service\n有的服务不需要k8s自带负载均衡和service ip，可以指定`spec.clusterIP`为None。这样这个service就不会被kube-proxy处理，平台也不会进行负载均衡，DNS的配置取决于selector。\n\n# 服务类型\nKubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP 类型。\n\nType 的取值以及行为如下：\n\n* ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。\n* NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 <NodeIP>:<NodePort>，可以从集群的外部访问一个 NodePort 服务。\n* LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。\n* ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com）。 没有任何类型代理被创建，这只有 Kubernetes 1.7 或更高版本的 kube-dns 才支持。\n\n# 外部IP\n```\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: MyApp\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 9376\n  externalIPs: \n    - 80.11.12.10\n```\n\n# Ingress\n```\n    internet\n        |\n   [ Ingress ]\n   --|-----|--\n   [ Services ]\n\n```\nIngress是授权入站，达到集群服务的规则集合，可配置的包括，提供外部可访问的URL，负载均衡，SSL，基于名称的虚拟主机。Ingress同样是REST对象，可以通过POST到API server的方式请求。需要通过Ingress Controller（Traefik）实现","tags":["K8S"]},{"title":"K8S学习系列（二）：开放接口","url":"/2020/06/01/kubernetes_2/","content":"\nK8S学习系列（二）：开放接口\t\n===\n在K8S里，主要开放了三个接口，对接不同的后端\n* CRI（Container Runtime Interface）：容器运行时接口，提供计算资源\n* CNI（Container Network Interface）：容器网络接口，提供网络资源\n* CSI（Container Storage Interface）：容器存储接口，提供存储资源\n\n# CRI\nCRI定义了容器和镜像的服务的接口，因为容器运行时和镜像的生命周期彼此隔离，因此需要定义两个服务。定义于`pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto`。\n\n![avatar](https://jimmysong.io/kubernetes-handbook/images/cri-architecture.png)\n\n## 接口标准\nCRI的services包括RuntimeService和ImageService。而Kubelet则通过grpc Client运行，以grpc与k8s通信。\n\n* RuntimeService：容器和Sandbox运行时管理。\n* ImageService：提供了从镜像仓库拉取、查看、和移除镜像的RPC。\n\n主要的接口如下所示，在这里花大篇幅贴出接口的代码也是可以让读者更好地理解这些接口到底做了什么。\n```\n// Runtime service defines the public APIs for remote container runtimes\nservice RuntimeService {\n    // Version returns the runtime name, runtime version, and runtime API version.\n    rpc Version(VersionRequest) returns (VersionResponse) {}\n\n    // RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure\n    // the sandbox is in the ready state on success.\n    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}\n    // StopPodSandbox stops any running process that is part of the sandbox and\n    // reclaims network resources (e.g., IP addresses) allocated to the sandbox.\n    // If there are any running containers in the sandbox, they must be forcibly\n    // terminated.\n    // This call is idempotent, and must not return an error if all relevant\n    // resources have already been reclaimed. kubelet will call StopPodSandbox\n    // at least once before calling RemovePodSandbox. It will also attempt to\n    // reclaim resources eagerly, as soon as a sandbox is not needed. Hence,\n    // multiple StopPodSandbox calls are expected.\n    rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}\n    // RemovePodSandbox removes the sandbox. If there are any running containers\n    // in the sandbox, they must be forcibly terminated and removed.\n    // This call is idempotent, and must not return an error if the sandbox has\n    // already been removed.\n    rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}\n    // PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not\n    // present, returns an error.\n    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}\n    // ListPodSandbox returns a list of PodSandboxes.\n    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}\n\n    // CreateContainer creates a new container in specified PodSandbox\n    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}\n    // StartContainer starts the container.\n    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}\n    // StopContainer stops a running container with a grace period (i.e., timeout).\n    // This call is idempotent, and must not return an error if the container has\n    // already been stopped.\n    // TODO: what must the runtime do after the grace period is reached?\n    rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}\n    // RemoveContainer removes the container. If the container is running, the\n    // container must be forcibly removed.\n    // This call is idempotent, and must not return an error if the container has\n    // already been removed.\n    rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}\n    // ListContainers lists all containers by filters.\n    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}\n    // ContainerStatus returns status of the container. If the container is not\n    // present, returns an error.\n    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}\n    // UpdateContainerResources updates ContainerConfig of the container.\n    rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {}\n\n    // ExecSync runs a command in a container synchronously.\n    rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {}\n    // Exec prepares a streaming endpoint to execute a command in the container.\n    rpc Exec(ExecRequest) returns (ExecResponse) {}\n    // Attach prepares a streaming endpoint to attach to a running container.\n    rpc Attach(AttachRequest) returns (AttachResponse) {}\n    // PortForward prepares a streaming endpoint to forward ports from a PodSandbox.\n    rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {}\n\n    // ContainerStats returns stats of the container. If the container does not\n    // exist, the call returns an error.\n    rpc ContainerStats(ContainerStatsRequest) returns (ContainerStatsResponse) {}\n    // ListContainerStats returns stats of all running containers.\n    rpc ListContainerStats(ListContainerStatsRequest) returns (ListContainerStatsResponse) {}\n\n    // UpdateRuntimeConfig updates the runtime configuration based on the given request.\n    rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {}\n\n    // Status returns the status of the runtime.\n    rpc Status(StatusRequest) returns (StatusResponse) {}\n}\n\n// ImageService defines the public APIs for managing images.\nservice ImageService {\n    // ListImages lists existing images.\n    rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {}\n    // ImageStatus returns the status of the image. If the image is not\n    // present, returns a response with ImageStatusResponse.Image set to\n    // nil.\n    rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {}\n    // PullImage pulls an image with authentication config.\n    rpc PullImage(PullImageRequest) returns (PullImageResponse) {}\n    // RemoveImage removes the image.\n    // This call is idempotent, and must not return an error if the image has\n    // already been removed.\n    rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {}\n    // ImageFSInfo returns information of the filesystem that is used to store images.\n    rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {}\n}\n```\n## 支持容器运行时\nCRI可以支持不同的容器运行时，包括cri-o，cri-containerd，rkt，frakti，docker。CRI由[SIG-Node](https://kubernetes.slack.com/archives/sig-node)维护。\n\n有一部分容器只实现了OCI标准，可以通过CRI-O来将自己作为K8S的容器运行时。包括：\n* Clear Containers\n* Kata Containers\n* gVsior\n\n# CNI\nCNI（Container Network Interface）是CNCF旗下的一个项目，由一组用于配置Linux容器的网络接口的规范和库组成，同时包含了一些插件。CNI是三个接口中最复杂的一个，其模型图如下所示：\n![avator](https://img-blog.csdnimg.cn/20190815153903714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RleHRkZW1vMTIz,size_16,color_FFFFFF,t_70)\n\n它主要解决的问题是连接了网络插件和容器管理系统。每个容器内部有自己的网络栈，包括对容器的接口，路由表以及DNS配置的管理。也就是在Docker中隔离容器的Network Namespace。而如果想让它和外界网络交流，则需要其进行网络栈的配置。通过CNI，kubelet启动infra容器以后，就可以通过调用CNI直接配置好容器的Network Namespace的网络栈（包括网卡（Network Interface、回环设备（Loopback Device）、路由表（Routing Table）和iptables规则）。我们通过实现创建网络的网络方案，之后再通过实现对应的插件，将容器加入网络。\n\n上知乎找了张图，大致流程就是这样。\n![avator](https://pic1.zhimg.com/v2-1e6f56f9c44c07a129176cc71c6986fc_b.jpg)\n\n## 接口定义\n下面是CNI定义的一系列控制容器网络配置的接口。\n```\ntype CNI interface {\n    AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)\n    DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error\n\n    AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error)\n    DelNetwork(net *NetworkConfig, rt *RuntimeConf) error\n}\n```\n## 可用插件\n### Main：接口创建\n* bridge：创建网桥，并添加主机和容器到该网桥\n* ipvlan：在容器中添加一个ipvlan接口\n* loopback：创建一个回环接口\n* macvlan：创建一个新的MAC地址，将所有的流量转发到容器\n* ptp：创建veth对\n* vlan：分配一个vlan设备\n### IPAM：IP地址分配\n* dhcp：在主机上运行守护程序，代表容器发出DHCP请求\n* host-local：维护分配IP的本地数据库\n### Meta：其它插件\n* flannel：根据flannel的配置文件创建接口\n* tuning：调整现有接口的sysctl参数\n* portmap：一个基于iptables的portmapping插件。将端口从主机的地址空间映射到容器。\n\n# CSI\nCSI（Container Storage Interface）代表容器存储接口。CSI借助CSI容器编排系统，可以将任意存储系统暴露给容器工作负载。\n\n## 预配置\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-manually-created-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  csi:\n    driver: com.example.team/csi-driver\n    volumeHandle: existingVolumeName\n    readOnly: false\n```\n\n## 附着和挂载\n```\nkind: Pod\napiVersion: v1\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: my-frontend\n      image: dockerfile/nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: my-csi-volume\n  volumes:\n    - name: my-csi-volume\n      persistentVolumeClaim:\n        claimName: my-request-for-storage\n```","tags":["K8S"]},{"title":"K8S学习系列（一）：K8S的基本设计","url":"/2020/05/29/kubernetes_1/","content":"\nKubernetes的设计理念\n===\nKubernetes作为目前容器编排领域的几乎垄断型选手，在学习Cloud Native方面是很重要的一环，因此今天开个系列，来记录一下K8S的学习过程。今天主要的内容还是搞清楚K8S的架构和大体上的各个组件和基本功能，之后如果有时间再深入其他的缓解，主要的参考资料是 `https://jimmysong.io/kubernetes-handbook`\n\n# Kubernetes的基本结构\n借用handbook里面的一张图，就描述了整个Kubernetes的大致架构和各个组件之间的交互。主要的部分有两个，一个就是master component，一个是Node。\n![avatar](https://jimmysong.io/kubernetes-handbook/images/kubernetes-high-level-component-archtecture.jpg)\n\n在Master Component上，主要的组件是etcd， scheduler，controller manager和api server，负责集群状态管理，资源调度，负载均衡等等。而Node也就是K8S真正要管理的资源单位。一个Node下面可以有多个pod。具体的细节我们在本章节后面讨论。在这里本人有个思考，因为kubernetes被称为云操作系统，而Linux则是本地操作系统,如果能将两个系统对比,会不会有比较因缺思厅的东西发生呢?于是我找了两个分层结构图的对比:\n\nLinux的分层结构：\n\n![avatar](https://tecadmin.net/tutorial/wp-content/uploads/2017/10/linux-architecture-image.png)\n\nK8S的分层结构：\n![avatar](https://jimmysong.io/kubernetes-handbook/images/kubernetes-layers-arch.png)\n\nK8S的结构分为核心层（对外提供API构建应用，对内提供插件式应用），应用层（部署，路由和servicemesh部分），管理层（系统度量，自动化，policy和servicemesh部分），接口层（命令行工具）和生态系统（包括外部的集成应用和内部的资源CRI、CNI、CSI、image registry和cloud provider等）。\n\n如果我们把这个对应到linux的图中，应该是Hardware对应K8S调配的资源，Kernal对应API层，也就是一系列的system library帮助我们在资源上搭建应用，本质上来说，K8S的应用层和管理也应该属于Kernal，因为它做的是一些networking和scheduler的工作。然后到了接口层，很显而易见的对应shell，最后在此之上搭建了appliction。\n\n总结一下，两个模型对应关系分别是Hardware->Cloud Computing Resource，Kernal对应核心层，应用层加管理层，Shell对应接口层，Application对应应用层。\n\n# 设计原则\n\n## API\n在K8S集群中，每添加一项新功能，就等于引入了对应的API对象。API设计的原则主要有如下几条：\n\n* **声明式。** 声明式同时也是Cloud Native的一个原则，目的就是防止系统状态出现不一，同时隐藏系统具体完成API的过程。比较典型的一个例子就是replica的数量。如果是声明式，那就直接告诉系统replica是3个，如果是命令式，则需要告诉系统从x到3的过程，包括增减replica。前者的好处就是API调用的过程中已经包含了调用者想要系统最后达到的状态，有助于系统状态的维护，分布式里非常重要。\n\n* **互补可组合。** 对于这一点的理解就是“高内聚，松耦合”。提高对象可重用性。\n\n* **高层API从操作意图出发。** 这一点我的理解就是API的设计需要和操作的意图相结合，让你能够将操作对应到API上，而不是需要比较复杂的API组合才能完成一个操作。\n\n* **低层API根据高层控制需要设计。** 低层API由于不需要被用户直接使用，则是需要根据被高层使用时的可重用性，减少冗余方面考虑。\n\n* **避免简单封装。** 这一点，我的理解是，对于一些封装逻辑很简单的API我们没必要强行组合它们。我们需要让操作者知道这个API对什么对象做了什么事情，而不是简单封装它们而导致API意图不清。\n\n* **API操作复杂度。** 这点很好理解，毕竟是Cloud Native，伸缩性和水平扩展，API操作复杂度不能超过O(N)，不然就没有意义了。\n\n* **API对象状态不依赖网络状态。** API对象在分布式的环境下，即使遭遇网络断开，也能保持稳定的状态。（状态包括？）\n\n* **操作机制不依赖全局状态。** 毕竟分布式环境下全局状态完全同步比较困难，即使还未同步我们也希望它能可用。\n\n## 控制机制设计原则\n* 只依赖当前状态\n* 假设任何错误的可能，并做容错物理\n* 控制逻辑不能依赖无法监控的内部状态（意思就是可控的，不能说一个子系统状态变化对另一个子系统的状态影响是不可预测的）\n* 假设任何操作都可能被任何对象拒绝或者错误解析。（即使这样，也不能影响系统的稳定性）\n* 模块自我修复能力\n* 模块优雅降级（保证基本服务可用性）\n\n# 核心技术概念\n\n## API对象\n每个功能对应的会有API对象，比如Replica Set对应RS。每个API对象有三大类属性：metadata， spec和status。metadata至少包括namespace，name和uid，还有labels标识对象类别，比如env标识不同环境下的API对象。Spec描述了理想状态，status描述了当前状态。比如pod副本数理想状态是3，当前状态是2。下面是一些比较重要的API对象\n\n## Pod\nPod是K8S中运行部署应用或服务的最小单元，支持多容器。多个容器会在一个pod下面共享网络服务和文件系统。比如一个对外发布的网络服务，一个可能会向外发布，一个可能会拉取新的信息，两个容器被不同的team开发，最终放在一个pod下组成一个微服务。\n\n## Replication Controller（RC）\nRC是最早保证Pod高可用的API对象，在K8S比较早期实现，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。\n\n## Replica Set（RS）\nRS是新一代RC，同样提供高可用，区别是提供更多匹配样式。一般作为Deployment的理想参数使用。\n\n## Deployment\n部署其实是一个复合操作，包括更新新服务，创建新服务，滚动升级等。滚动升级相当于是RS将新服务的副本数目增长，减少旧服务这样一个复合操作。\n\n## Service\n因为Pod本身所在的物理位置是并不确定的，因此Service提供了服务发现和负载均衡的能力，在Pod更新的过程中同时进行服务IP和端口的更新，保证高可用。负载均衡在K8S中是通过Kube-proxy实现的，一个Node上面一个。\n\n## Job\nJob是K8S用来控制批处理型任务的API对象。批处理的任务是有头有尾的。成功完成的标志根据不同的spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。\n\n## DaemonSet\n后台服务主要要保证每个Node都有一个此类的Pod在运行，包括存储，日志和监控等。\n\n## StatefulSet\n有状态服务集，主要是针对一些有状态的应用，即，即使Pod挂了，我们也希望用同样的名字将其恢复到原来的状态，包括挂载同样的存储（意味着主要使用外挂存储），主要针对Mysql等DB服务，Zookeeper，etcd等状态管理服务等。同时可以作为一种虚拟机保存，因为Container的状态保存其实是很不安全和不可靠的。\n\n## Federation\n主要针对跨地区的kubernetes cluster再进行集群管理，K8S cluster可以注册成为联邦的一份子。不影响cluster内部的逻辑。\n\n## Volume\nKubernetes集群中的存储卷跟Docker的存储卷有些类似，只不过Docker的存储卷作用范围为一个容器，而Kubernetes的存储卷的生命周期和作用范围是一个Pod。每个Pod中声明的存储卷由Pod中的所有容器共享。\n\n## Persistent Volume / Persistent Volume Claim\nPV和PVC的关系与Node和Pod的关系类似。PV和Node是资源提供者，PVC和Pod是资源使用者。PV和PVC使得K8S cluster的存储抽象化。\n\n## Node\nNode就是Pod所在的主机，无论是物理机还是虚拟机。\n\n## Secret\nSecret对象用来传递密码，密钥，证书等。\n\n## User Account / Service Account\nUser Account标识用户，Service Account标识服务。\n\n## Namespace\nNamespace为K8S Cluster提供虚拟隔离，初始namespace包括default和kube-system。\n\n## RBAC\n权限控制，不必多说，所有权限控制系统都叫这个名字。\n","tags":["K8S"]},{"title":"手撕Docker系列-第一章","url":"/2020/04/25/chapter-1/","content":"\n手撕Docker系列-第一章\n===\n之前看到第一章是介绍go和docker相关知识就没有过多深入，今天有空看了看，觉得一些东西还是值得记录，因此写了这一章。\n\n# Docker\n\n能看到这篇文章的想必对docker也有一定了解，不必要的话就无需讲了。Docker作为virtualization的核心技术，可以将应用程序以及相应的依赖资源打包成一个标准的镜像，并以容器的方式运行在任何支持docker engine的系统上。Docker总体采用的是典型的C/S结构，并没有涉及很多底层或者分布式系统的东西。具体架构图如下。\n\n![avatar](https://wiki.aquasec.com/download/attachments/2854889/Docker_Architecture.png?version=1&modificationDate=1520172700553&api=v2)\n\n可以看到比较核心的是Docker Deamon，此外客户端和docker的交互就是通过api的形式进行。具体Deamon和其他组件做了什么，在后面的章节我们具体学习。\n\nDocker的优势主要有三个，\n\n- 轻量级：在同一台宿主机的容器共享系统的kernal，我们无需再搭建一个OS，因此启动速度快（秒级启动），占用系统内存少。又因为AUFS使得镜像之间能够通过分层结构共享文件，提高了磁盘的利用率和镜像下载速度。\n- 开放：Docker容器基于开放标准，因此Docker可以在主流Linux和windows操作系统上运行。\n- 安全：通过Namespace达到了资源的隔离，docker之间无法相互干扰，提供了额外的保障机制。\n\n# Docker和VM\n同样是模拟出一个独立的操作系统环境，VM虚拟机经常拿来和Docker进行比较。\n![avatar](https://wiki.aquasec.com/download/attachments/2854889/Container_VM_Implementation.png?version=1&modificationDate=1520172703952&api=v2)\n\n这个图就很明显了。VM的核心技术集中于Hypervisor上，Hypervisor更像是一个软件，基于OS的基础上，模拟出各种硬件的行为（包括CPU，硬盘等）。在Hypervisor的基础上，我们再搭建OS。这个缺点很明显，就是我们每需要打开一个虚拟机，我们就需要在Hypervisor上安装一个OS，动辄就是几个GB。\n\n而Docker克服了VM的缺点，对开发人员开发效率来说，主要有三个帮助：\n\n- 加速开发：Docker Registry提供了各种标准化的镜像，同时也提供了开发者定制镜像的能力，再也无需花费很久重新设置环境。\n- 赋能创造力：对这一点，我理解为，由于启动一个docker成本低而且docker之间相互不干扰，使得我们可以为每一个程序设置最好的环境，避免依赖之间相互冲突，以及复杂的版本管理。\n- 消除环境不一致：Docker的标准使得我们开发不用受开发环境的限制，无论在什么环境下达到开箱即用的效果。\n\n此外，Docker Hub的存在使得一个团队可以通过共享镜像的方式实现协作开发。此外，docker秒级启动的特性能让服务迅速扩容。","tags":["Docker"]},{"title":"手撕Docker系列-第三章","url":"/2020/04/25/chapter-3/","content":"\n手撕Docker系列-第三章\n===\n今天在这个章节我们会讲如何应用我们前一章的知识，来创建一个容器进程。\n\n# Linux proc文件系统\n\nLinux的/proc目录其实不是一个真正的文件系统，因为真正的文件系统通过一系列的metadata对磁盘上的文件进行管理。而/proc下的内容包含了系统runtime的metadata，包括系统内存，mount设备以及一些硬件配置。它只存在于内存中，而不占用外存空间。事实上，它只是提供了一个接口让用户以访问文件的形式访问这些信息。\n\n|  目录   | 内容  |\n|  ----  | ----  |\n| /proc/N | PID为N的进程信息 |\n| /proc/N/cmdline | 进程启动命令 |\n| /proc/N/cwd | 链接到进程当前工作目录 |\n| /proc/N/environ | 进程环境变量列表 |\n| /proc/N/exe | 链接到进程的执行命令文件 |\n| /proc/N/fd | 包含进程相关的所有文件描述符 |\n| /proc/N/maps | 与进程相关的内存映射信息 |\n| /proc/N/mem | 指代进程持有的内存(不可读) |\n| /proc/N/root | 连接到进程的根目录 |\n| /proc/N/stat | 进程状态 |\n| /proc/N/statm | 进程使用的进程状态 |\n| /proc/N/status | 进程状态信息，比stat/statm更具可读性 |\n| /proc/self/ | 链接到当前正在运行的进程 |\n\n# Run命令启动\n\nmydocker\n├─README.md\n├─main.go\n├─main_command.go\n├─run.go\n├─network\n&nbsp;|&emsp;└test_linux.go\n├─container\n&nbsp;|&emsp;├─container_process.go\n&nbsp;|&emsp;└init.go\n├─Godeps\n&nbsp;|&emsp;├─Godeps.json\n&nbsp;|&emsp;└Readme\n\n文件目录如上所示。在这里我们不贴出过多的细节，主要把容器的启动过程分为几个部分。\n\n## run\nrun的过程里，最主要的定义了一些运行的flag（tty）。`Run(tty, cmd)`是关键的部分，这部分代码有必要贴出来。\n\n```\nfunc Run(tty bool, command string) {\n\tparent := container.NewParentProcess(tty, command)\n\tif err := parent.Start(); err != nil {\n\t\tlog.Error(err)\n\t}\n\tparent.Wait()\n\tos.Exit(-1)\n}\n\nfunc NewParentProcess(tty bool, command string) *exec.Cmd {\n\targs := []string{\"init\", command}\n\tcmd := exec.Command(\"/proc/self/exe\", args...)\n    cmd.SysProcAttr = &syscall.SysProcAttr{\n        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS |\n\t\tsyscall.CLONE_NEWNET | syscall.CLONE_NEWIPC,\n    }\n\tif tty {\n\t\tcmd.Stdin = os.Stdin\n\t\tcmd.Stdout = os.Stdout\n\t\tcmd.Stderr = os.Stderr\n\t}\n\treturn cmd\n}\n```\n在`container.NewParentProcess(tty, command)`中，mydocker启动了一个进程`/proc/self/exe`，这个在我们之前的段落已经有提及，就是设置了一个进程变量，进程的可执行文件为当前进程的可执行文件。还有一些第二章提到的Namespace来进行资源隔离。包括UTS（hostname），PID（进程号），NS（挂载点），NET（网络资源）和IPC（进程交互通道）。在之后又设置了一些输入输出的通道。可能看到它调用自己会有点疑惑，其实更清楚的思路是，在创建这个进程的时候也传入了一个参数`init`，相当于调用./mydocker init。我们再看看init干了什么。\n\n## init\n在init的过程中，主要是init了container的process，最关键的一个函数叫做`container.RunContainerinitProcess(cmd, nil)`。\n\n```\nfunc RunContainerInitProcess(command string, args []string) error {\n\tlogrus.Infof(\"command %s\", command)\n\n\tdefaultMountFlags := syscall.MS_NOEXEC | syscall.MS_NOSUID | syscall.MS_NODEV\n\tsyscall.Mount(\"proc\", \"/proc\", \"proc\", uintptr(defaultMountFlags), \"\")\n\targv := []string{command}\n\tif err := syscall.Exec(command, argv, os.Environ()); err != nil {\n\t\tlogrus.Errorf(err.Error())\n\t}\n\treturn nil\n}\n```\n\n在这个函数里创建的进程本质上没有干什么事情。而函数本身首先规定了一个挂载参数，然后挂载到/proc上。之后再通过exec启动。\n\nMountFlags的意义如下\n* MS_NOEXEC： 这个文件系统中不允许运行其它程序。\n* MS_NOSUID: 这个文件系统下不允许设置user_id或者group id。\n* MS_NODEV: Linux2.0默认设置参数。\n\nexec这句语句也很关键，看起来它只是简单执行了一个程序。但是其实背后有比较复杂的逻辑。首先，当我们运行完run命令之后，我们希望暴露给我们的前台进程是容器进程，然后目前为止PID为1的前台进程仍然是init进程，而syscall.Exec这个方法， 其实最终调用了Kernel的intexecve(const char filename,char *const argv[], char *const envp[]);这个系统函数会执行对应文件，并覆盖当前进程的镜像，堆栈和数据，包括PID。\n\n在之后，我们容器已经启动了，我们可以通过`ps -ef`去查看目前的进程号是否为1。\n\n# 增加容器资源限制\n在这个部分，我们希望能够让mydocker实现资源限制。e.g. `mydocker run -ti -m IOOm -cpuset I -cpushare 512 /bin/sh`。其实在经历过第二章原理以后，这部分的实现相当容易。首先，假设我们在这里只考虑memory的限制，我们要做的就是创建一个memory subsystem。在这个memory subsystem中，我们将限制的变量写入memory挂载点下面指定的Cgroup。具体的subsystem的挂载点可以通过/proc/self/mountinfo来进行查看，值得注意的是，mountinfo里面得到的并不是文件系统下的绝对路径，我们仍然需要通过拼接等得到绝对路径。之后将限制参数写入Cgroup下的文件里。最后我们通过将进程加入挂载点下的指定Cgroup中来限制进程资源的使用。具体的代码实现可以看书中的实现\n\n![avatar](https://docs.google.com/drawings/d/e/2PACX-1vR4tG0VAHY4bizgADLvOKWP_olEh5NMrS0_D0BeMQVmx7gabMJaqdB8xrtg_RqQunad32VTwAkCG5iw/pub?w=960&h=720)\n\n# 增加管道和环境变量识别\n首先在这个章节我们考虑一个问题，就是在容器中父子进程的通信。其实在初始化的过程中，父子进程就已经有一次通讯了。也就是我们需要启动`mydocker init --args`的时候，我们传给子进程的参数，包括init command和参数。当出现参数太长或者有特殊字符串的时候，这种办法就会失败。事实上runC采用的就是匿名管道的方法进行通信。我们在这里需要增加一个函数`NewPipe()`。\n\n```\nfunc NewPipe() (*os.File, *os.File, error) {\n\tread, write, err := os.Pipe()\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn read, write, nil\n}\n\nfunc NewParentProcess(tty bool) (*exec.Cmd, *os.File) {\n\treadPipe, writePipe, err := NewPipe()\n\tif err != nil {\n\t\tlog.Errorf(\"New pipe error %v\", err)\n\t\treturn nil, nil\n\t}\n\tcmd := exec.Command(\"/proc/self/exe\", \"init\")\n\tcmd.SysProcAttr = &syscall.SysProcAttr{\n\t\tCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS |\n\t\t\tsyscall.CLONE_NEWNET | syscall.CLONE_NEWIPC,\n\t}\n\tif tty {\n\t\tcmd.Stdin = os.Stdin\n\t\tcmd.Stdout = os.Stdout\n\t\tcmd.Stderr = os.Stderr\n\t}\n\tcmd.ExtraFiles = []*os.File{readPipe}\n\treturn cmd, writePipe\n}\n```\n\nNewPipe通过os创建了一个匿名管道用于父子进程交互。readPipe作为了cmd.ExtraFiles参数传给了init进程，此时，一个进程便拥有了四个文件句柄：标准输入，标准输出，标准错误以及这个readPipe。而写句柄则是传到外部，并将需要运行的command写入，从而让init进程能够读到这些参数。在这之后writePipe就被close了。\n\n# 总结\n至此，我们为进程添加了隔离环境，资源限制以及管道机制，基本上实现了一个容器进程的运行。","tags":["Docker"]},{"title":"手撕Docker系列-第二章","url":"/2020/03/24/chapter-2/","content":"\n手撕Docker系列-第二章\n===\n终于开始写博客啦，希望能够通过这个过程让自己能够沉淀下来，更好地深入技术吧。因为最近入职于一家SAAS公司，平时工作里不可避免的要碰到k8s和docker，趁此机会也好好熟悉这两门技术。主要的学习手段是通过陈显鹭前辈的这本《自己动手写Docker》开始，一方面是实现一个东西可以让人对它了解更深刻，另一方面是熟悉go语言的使用，毕竟是工作语言...借此机会系统地学一下。\n\n这一章主要分为三个部分，一个是namespace，一个是Cgroup，还有就是AUFS。作为docker虚拟化概念中的核心技术。\n\n# Namespce\nNamespace是Linux的Kernel的一个功能，通过这个手段，Linux可以将其，我们在这里称之为各种资源隔离开来，其中包括进程，包括UserID，还有Network等。这也是docker镜像之间能够相互不干扰的原理所在。\n\nNamespace根据资源类型，我们可以将其分为好几种，具体的代码可以见我的github，也可以直接参考《自己动手写Docker》。在Linux中一共实现了6种不同类型的Namespace\n\n## UTS Namespace\n\nUTS Namespace用于隔离nodename和domainname，前者就是所谓的主机名，后者就是域名。在创建新的UTS namespace之后内部hostname独立于外部。\n\n## IPC Namespace\n\nIPC Namespace用于隔离System V IPC和POSIX message queues，前者就是Unix早期进程间通信的所有集合，包括管道（同时包括有名管道）、信号、消息队列、共享内存、信号量。后者是提供了实现POSIX标准的消息队列。\n\n## PID Namespace\n\nPID Namespace是用来隔离进程ID的。要注意这里是进程ID不是进程.同一个进程在不同PID Namespace可以拥有不同的PID。\n\n## Mount Namespace\n\nMount Namespace用来隔离各个进程看到的挂载点视图。首先比较难理解的是挂载，在这里，我将其解释为将文件系统和目录树结合在一起的一种结构，相当于将一个磁盘挂载至一个挂载点后，你就可以通过文件目录访问磁盘，并且文件系统也提供了inode、block等信息，更多的是一个关于怎么管理这片磁盘区域的配置信息。\n\n## User Namespace\n\nUser Namespace相对来说就好理解很多。众所周知，在Linux里面每个User有自己的UID和GroupID，后者规定了用户所在的用户组（权限控制相关）。因此User Namespace就是来划分这个UID和GroupID的。在不同的User Namespace中，不同的用户可以有不同的UID和GroupID，而且在不同User Namespace中的ID也是没有关联的，比如UID=1在两个User Namespace中就是完全相互独立的。\n\n## Network Namespace\n\n这个书里讲得很清楚。Network Namespace就是用来隔离网络设备，IP端口等网络栈的命名空间。每个Network Namespace中可以拥有独立的虚拟网络设备和自己的端口，且与其他的Network Namespace不冲突。\n\n# Cgroup\n\nLinux Cgroup，全程Control Group。它的出现为的是解决一个什么问题呢？之前的Namespace，针对的只是Namespace之间的隔离。而我们如果要求能够对资源进行限制呢？现有的Namespace是无法做到这点的。于是Linux中就引入了Cgroup这个概念。Cgroup针对的是进程，相当于是一个进程分组框架。在Cgroup中主要有两个概念：hierarchy和subsystem。\n\n## Hierarchy\n\nHierachy表达的是一个层次结构，Hierachy是一个树状结构，而在一个Hierachy下，可以绑定多个子Hierachy。这样就实现的进程的层级控制，所以一棵树更多的是相当于将进程进行分组。\n\n## Subsystem\n\n首先声明一个约束，一个subsystem只能挂载到一个Cgroup Hierachy节点上。而这个subsystem可以根据约束的资源，分为9种类型：\n+ cpu subsystem （CPU使用率）\n+ cpuacct subsystem（进程的CPU使用报告）\n+ cpuset subsystem（为进程分配单独的CPU节点或者内存节点）\n+ memory subsystem（内存分配）\n+ blkio subsystem（设备io资源分配）\n+ devices subsystem（设备访问控制）\n+ net_cls subsystem（标记Cgroup下的进程数据包，使用tc模块（traffic control）进行数据包控制）\n+ freezer subsystem （挂起恢复进程）\n+ ns subsystem （使得不同cgroup下面的进程使用不同的namespace）\n\n（net_cls和freezer还不是很懂）\n\n# AUFS\n之前的两个概念一个帮助docker容器之间相互隔离，一个帮助docker分配和限制系统资源。AUFS（Advanced Multi-Layered Unification Filesyste）则是用来高效节省空间和文件复用的。书中的cases由于编码的原因比较难理解，我在此会用比较简单的方式表达。首先docker会管理本地仓库的一堆images。比如，我pull了一个ubuntu 14.0.0的image 1，这个image由四层image layer组成。然后基于这个image，我又创建了一个docker image 2\n\n```\nFFROM image 1\n\nRUN sh-cmd\n```\n\n## Image Layer\n在这个sh-cmd中，有可能一些文件被更改了，但是其实大部分都是可以复用的，而在/var/lib/docker/aufs/diff/下面存了所有的layer，在这里，我们假设image 1有3个layer： layer1， layer2， layer3。而/var/lib/docker/aufs/layers则是存储了layer的metadata。比如在layer3下包括了layer1和layer2.在我们创建了新的image 2后。我们新添加了一个layer，而相比较image1，这个image其实并没有发生很大变大，因此我们只需要将他们的diff存在layer4，而image 2则由layer1，layer2，layer3，layer4组成。当访问不到相关文件，就会去下层的layer寻找。在书中的例子中，layer4的大小仅为12B大大节省了空间。\n\n## Container Layer\nContainer Layer则是用于管理container创建以后的管理.当一个container创建之后，会用到一个技术，被称为写时复制(copy on write)。也就是说，当且仅当这个container对文件进行写操作的时候，文件才会从下层Layer复制上来。而这个缺点则是，即使文件有很小的改动，也需要复制整个文件，好处就是，可以让文件尽可能的服用和节省磁盘空间。Container创建的时候会同事创建两个layer，一个是layer—id-init，另一个则是layer-id。前者是read-only的，存一些关于这个docker镜像的环境相关的数据，另一个则是read-write层，用于完成之前我提到的CoW技术。Container的metadata存在/var/lib/containers/container_id，包括容器的metadata和一些config。\n\n而关于删除一个文件file1，则是在read-write层添加一个.wh.file1，这样就可以屏蔽这个层以下所有的read-only层的file1文件。\n\n到此，这个章节也告一段落。","tags":["Docker"]},{"title":"MIT 6.828--Lab1","url":"/2019/04/30/MIT 6.828-Lab1/","content":"\n\n## Section 1\n\n\n\n\n## Section 2\n![Aaron Swartz](https://github.com/SwimmingFish6/MDImageResource/raw/master/elf.jpg)\n","tags":["OS"]},{"title":"Hello World","url":"/2019/04/30/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]